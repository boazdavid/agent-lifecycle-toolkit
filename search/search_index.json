{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ALTK","text":"Delivering plug-and-play, framework-agnostic technology to boost agents' performance Star us on GitHub!"},{"location":"#what-is-altk","title":"What is ALTK?","text":"<p>The Agent Lifecycle Toolkit helps agent builders create better performing agents by easily integrating our components into agent pipelines. The components help improve the performance of agents by addressing key gaps in various stages of the agent lifecycle, such as in reasoning, or tool calling errors, or output guardrails.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>To use ALTK, simply install agent-lifecycle-toolkit from your package manager, e.g. pip:</p> <pre><code>pip install agent-lifecycle-toolkit\n</code></pre> <p>More detailed installation instructions are available in the docs.</p>"},{"location":"#features","title":"Features","text":"Lifecycle Stage Component Purpose Pre-LLM Spotlight Does your agent not follow instructions? Emphasize important spans in prompts to steer LLM attention. Pre-tool Refraction Does your agent generate inconsistent tool sequences? Validate and repair tool call syntax to prevent execution failures. Pre-tool SPARC Is your agent calling tools with hallucinated arguments or struggling to choose the correct tools in the right order? Make sure tool calls match the tool specifications and request semantics, and are generated correctly based on the conversation. Post-tool JSON Processor Is your agent overwhelmed with large JSON payloads in its context? Generate code on the fly to extract relevant data in JSON tool responses. Post-tool Silent Error Review Is your agent ignoring subtle semantic tool errors? Detect silent errors in tool responses and assess relevance, accuracy, and completeness. Post-tool RAG Repair Is your agent not able to recover from tool call failures? Repair failed tool calls using domain-specific documents via Retrieval-Augmented Generation. Pre-response Policy Guard Does your agent return responses that violate policies or instructions? Ensure agent outputs comply with defined policies and repairs them if needed."},{"location":"concepts/lifecycle/","title":"Lifecycle Stages","text":"<p>A typical agent flow consists of three main stages: reasoning, acting, and interacting with the user (input and output processing). This introduces multiple points (see blue boxes in the figure below) to inject additional components that can address limitations in existing agents and boost their performance.</p> <p></p>"},{"location":"concepts/lifecycle/#post-request","title":"Post-Request","text":"<p>This stage comes immediately after a user request is received. It prepares the input for the agent and can include components like jail-breaking or other input guardrails.</p>"},{"location":"concepts/lifecycle/#pre-llm","title":"Pre-LLM","text":"<p>This stage occurs before the prompt is sent to the language model (LLM). It allows for prompt optimization, augmentation, or injection of additional context, etc. Unlike the post-request stage, the pre-llm stage can happen multiple times in the agent loop.</p> <p>ALTK includes one component in this stage: Spotlight.</p>"},{"location":"concepts/lifecycle/#reason-call-llms","title":"Reason (Call LLMs)","text":"<p>In this stage we often call LLMs to \"think\" or reason about the task or request they have received via a prompt.</p>"},{"location":"concepts/lifecycle/#post-llm","title":"Post-LLM","text":"<p>This stage occurs after the LLM returns a response. This may include the tool the LLM wants to call or generated text that includes the final answer (for non-tool call queries).</p> <p>Components in this stage can address format inconsistencies, response parsing or transformation before tool invocation or final output.</p>"},{"location":"concepts/lifecycle/#pre-tool","title":"Pre-Tool","text":"<p>This stage is triggered before a tool is called by the agent. Its purpose is to validates tool parameters, enforces policies (e.g., rate limits), and optionally intercepts or redirects tool calls.</p> <p>Failure classes components in this stage may address include invalid tool arguments, redundant or expensive calls, etc.</p> <p>ALTK includes two components in this stage: Refraction and SPARC.</p>"},{"location":"concepts/lifecycle/#act-call-tools","title":"Act (Call tools)","text":"<p>In this stage a tool or tools that the LLM has formulated to address the user's query is being invoked.</p>"},{"location":"concepts/lifecycle/#post-tool","title":"Post-Tool","text":"<p>This stage occurs after a tool is called (whether the tool call was successful or not). This allows for result validation, processing, caching, logging, and integration into the agent\u2019s response.</p> <p>Failure classes components in this stage may address include tool execution errors, response processing errors, etc.</p> <p>ALTK includes the following components for this stage: Silent Review, JSON Processor, RAG Repair.</p>"},{"location":"concepts/lifecycle/#pre-response","title":"Pre-Response","text":"<p>This is the final stage before the agent sends a response back to the user. Its purpose is to assemble the final output, apply formatting, and ensure compliance with response policies. Similar to the post-request stage, this stage occurs once outside the agent loop.</p> <p>Failure classes components in this stage may address include output formatting errors, missing information, response safety, violated policy guardrails, etc.</p> <p>ALTK includes the following components for this stage: Policy Guard.</p>"},{"location":"concepts/components/json-processor/","title":"JSON Processor: Code Generation for JSON Tool Response Processing","text":"<p>If the agent calls tools which generate complex JSON objects as responses, this component will use LLM based Python code generation to process those responses and extract relevant information from them.</p> <p>See a demo of this component in action here.</p>"},{"location":"concepts/components/json-processor/#overview","title":"Overview","text":"<p>The most common pattern for tool calling is to call a tool that returns some information. The agent then needs to process the tool response and extract the information from it. This information may be required either to respond back to the user or for the next step of the agent. When the tool responses are in JSON format, and the agent knows what information is needed from it, the code generation component prompts a LLM to generate Python code for parsing the JSON and extracting the required information. It executes the generated code and returns back the extracted content if the code execution succeeds.</p> <p>The figures below show the flow of tool calling with the code generation based JSON processor component:</p> <p></p> <p></p>"},{"location":"concepts/components/json-processor/#architecture","title":"Architecture","text":"<p>The figure below shows how the JSON processor component processes the tool response to get the answer i.e. the information from the JSON object by calling an LLM using a prompt that includes the JSON response and a natural language query.</p> <p>Given a user query and a tool response (as a JSON object), we 1. generate a schema from the JSON object (see and example below) using existing Python libraries such as genson, 2. determine if the response can fit in the prompt, and 3. if it is too long, we condense the response using a heuristic algorithm.</p> <p>We construct the prompt with the above inputs and pass it to an LLM that generates Python code to process the JSON object (see example below). The generated code is executed by a Python interpreter in a sandbox (for security reasons) to generate the final answer to the user's query.</p> <p></p>"},{"location":"concepts/components/json-processor/#sample-schema","title":"Sample Schema","text":"<pre><code>```python\nschema = {\n    \"rating_info\":{\n        \"type\": \"object\",\n        \"properties\": {\n            \"no_of_ratings\":{ \"type\": \"number\" },\n            \"dropoff_time\":{ \"type\": \"number\" },\n            \"location\":{ \"type\": \"number\" },\n            \"cleanliness\": { \"type\": \"number\" }\n        }\n    }\n</code></pre> <p>}     ```</p>"},{"location":"concepts/components/json-processor/#sample-llm-generated-python-code","title":"Sample LLM generated Python code","text":"<pre><code>python\\ndef get_cleanliness_rating(api_response):\\n    if not isinstance(api_response, dict):\\n        return \"\"\\n    \\n    data = api_response.get(\"data\")\\n    if not isinstance(data, dict):\\n        return \"\"\\n    \\n    search_results = data.get(\"search_results\")\\n    if not isinstance(search_results, list):\\n        return \"\"\\n    \\n    for result in search_results:\\n        if not isinstance(result, dict):\\n            continue\\n        \\n        vehicle_id = result.get(\"vehicle_id\")\\n        if vehicle_id and str(vehicle_id).strip().lower() == \"650948971\":\\n            rating_info = result.get(\"rating_info\")\\n            if isinstance(rating_info, dict):\\n                cleanliness = rating_info.get(\"cleanliness\")\\n                if cleanliness is not None:\\n                    return str(cleanliness)\\n    \\n    return \"\"\\n\n</code></pre>"},{"location":"concepts/components/json-processor/#interface","title":"Interface","text":"<p>This component expects the following inputs and generates the following outputs.</p>"},{"location":"concepts/components/json-processor/#input","title":"Input","text":"<ol> <li> <p><code>nl_query</code>: this is the natural language description (of datatype <code>str</code>) hinting at what information needs to be extracted from the response. It can be the agent's thought corresponding to the tool call or the user query directly.</p> <p>Example: <code>What is the cleanliness rating of 650948971?</code></p> </li> <li> <p><code>tool_response</code>: this is the JSON object from the tool (of datatype <code>Any</code>). A portion of the JSON response is shown below.</p> <p>Example: <pre><code> json_response = {\n     \"search_results\": [\n         {\n             \"vehicle_id\": \"657356548\",\n             ...\n         },\n         {\n              \"vehicle_id\": \"650948971\",\n               \"rating_info\": {\n                     \"value_for_money\": 8.1,\n                     \"cleanliness\": 8.5,\n                     \"efficiency\": 8.3,\n                     \"average_text\": \"Excellent\",\n                     \"condition\": 8.2,\n                     \"average\": 8\n                 }\n           }\n     ]\n }\n</code></pre></p> </li> </ol>"},{"location":"concepts/components/json-processor/#output","title":"Output","text":"<ol> <li> <p><code>answer</code>: this is the final obtained by executing the generated Python code on the JSON object.</p> <p>Example: <code>8.5</code> (based on the example above)</p> </li> </ol>"},{"location":"concepts/components/json-processor/#results","title":"Results","text":"<p>We evaluate our approach on a question-answer (QA) dataset containing 1298 samples (580 extractive, 394 filtering and 324 aggregation).</p> <p>Models' performance degraded as the JSON response increased in length, as shown in the figure below.</p> <p></p> <p>Across model families and sizes, adopting our JSON processor (red bars in the figure below) approach leads to accuracy gains ranging from +3% to +50% depending on the model compared to adopting a direct prompting approach that prompts the model to generate answer directly.</p> <p>Additional results are reported in [1].</p> <p></p>"},{"location":"concepts/components/json-processor/#getting-started","title":"Getting Started","text":"<p>Refer to this README for instructions on how to get started with the code. See an example in action here.</p>"},{"location":"concepts/components/json-processor/#references","title":"References","text":"<p>[1] Kate, K., et al., \"How Good Are LLMs at Processing Tool Outputs?,\" arXiv preprint arXiv: (2025).  https://arxiv.org/pdf/2510.15955</p> <p>[2] Kate, K., et al. \"LongFuncEval: Measuring the effectiveness of long context models for function calling.\" arXiv preprint arXiv:2505.10570 (2025). https://arxiv.org/pdf/2505.10570</p>"},{"location":"concepts/components/policy-guard/","title":"Policy Guard","text":"<p>Lifecycle components to guard and enforce policies (or instructions) in LLM responses.</p>"},{"location":"concepts/components/policy-guard/#overview","title":"Overview","text":"<p>Policy Guards are post-inference components that can be used to guard for and reinforce policies on LLM responses. A policy can be any statement or instruction that describes a constraint on an LLM response. For example: \"The response must be in JSON format\" or \"The response should not include any hyperboles\".</p> <p>A policy guard takes as input an LLM response and a set of policy statements and produces an output depending on its configuration:</p> <ul> <li>Detect Guard: The guard checks whether the LLM response follows the policy and outputs \"Yes/No\" along with an explanation.</li> <li>Detect+Repair Guard: The guard detects and repairs any policy violations in the LLM response; it outputs a rewritten and repaired version of the response.</li> </ul> <p>Detect and Repair are LLM-based and follow a specific strategy. The middleware components in this repo support the following detect and repair strategies:</p>"},{"location":"concepts/components/policy-guard/#detect-strategies","title":"Detect Strategies","text":"<ul> <li>Batch: all input policies are checked in a single LLM inference</li> <li>Single: input policies are checked one at a time in a series of LLM inferences</li> </ul>"},{"location":"concepts/components/policy-guard/#repair-strategies","title":"Repair Strategies","text":"<ul> <li>Batch: the response is rewritten once to repair a set of policy violations in a single LLM inference</li> <li>Iterative: the response is iteratively rewritten by repairing one policy violation in each iteration.</li> <li>Retry: retry repeatedly attempts to batch repair the response until no more violations are detected or the maximum number of retries has been reached.</li> <li>Best-of-n: best-of-n samples n batch repairs using temperature and selects the repaired sample with the fewest number of detected violations.</li> <li>MapReduce: in the map phase, repaired versions of the LLM response are generated independently for each detected policy violation. During the reduce phase all repaired versions are merged into a single repaired version.</li> </ul> <p>Experimentally, Best-of-N yields the strongest results with the best repair rate. Batch is the most cost-effective strategy as it requires only a single LLM inference for repair.</p>"},{"location":"concepts/components/policy-guard/#architecture","title":"Architecture","text":"<p>The answer is passed to the output guardrail block to enforce policy guardrails.</p> <p> </p>"},{"location":"concepts/components/policy-guard/#results","title":"Results","text":"<p>We evaluated policy guards by measuring the increase in the policy compliance rate that can be achieved with Policy Guards over simply adding the policy statements as instructions in the prompt.</p> <p>We used a derivative of the popular IFEval dataset, where we treated instructions as policy statements and scaled the number of instructions added to a query up to ten instructions.  We then measured the policy compliance rate as the instruction following (IF) rate without and with policy guards with using various strategies.</p> <p> </p> <p>In the above figure baseline refers to only adding the instructions to the prompt and Detect+Repair refers to the Batch repairer above. The above figure shows the achieved instruction following (IF) rates for the four strategies using Llama 3.1 70B. The IF rate achieved by only adding the instructions to the prompt is shown as the baseline. Detect+Repair in the figure above refers to the Batch repairer.  All repair strategies lead to IF rate improvements over the baseline. Even at two instructions, policy guards lead to small improvements and the benefits generally increase with the number of instructions. Best-of-N policy guards consistently provide the largest improvements, up to 4 percentage points at ten instructions, increasing the IF rate to 0.70 from 0.66. Best-of-N Oracle refers to a version where we used an oracle detector to select the best of the N generated version to illustrate the potential IF rate achievable through Best-of-N policy guards. Even at two instructions, the model is capable of generating repaired responses with an IF rate of 0.89, a 2 percentage point increase. The boost grows as instructions are increased to ten, when the IF rate reaches 0.75, an 8.5 percentage point increase.</p> <p>Additional results are reported in [1].</p>"},{"location":"concepts/components/policy-guard/#getting-started","title":"Getting Started","text":"<p>Refer to this README for instructions on how to get started with the code. See an example in action here</p>"},{"location":"concepts/components/policy-guard/#references","title":"References","text":"<p>[1] Elder, B., et al., \"Boosting Instruction Following at Scale,\" arXiv preprint arXiv: (2025).  https://arxiv.org/abs/2510.14842</p>"},{"location":"concepts/components/rag-repair/","title":"RAG Repair","text":"<p>Given a failing tool call, this component attempts to use an LLM to repair the call while making use of domain documents such as documentation or troubleshooting examples via RAG. This component will require a set of related documents to ingest.</p>"},{"location":"concepts/components/rag-repair/#overview","title":"Overview","text":"<p>The most common pattern for tool calling is to call a tool that returns some information. The agent then needs to process the tool response and extract the information from it. This information may be required either to respond back to the user or for the next step of the agent. Tool calls may fail for any number of reasons and resolution may require knowledge of the state of the external tool as well as domain knowledge. This tool attempts to use both types of knowledge to repair tool calls by using the tool call response along with retrieving related documentation via RAG.</p>"},{"location":"concepts/components/rag-repair/#architecture","title":"Architecture","text":"<p>The figure below shows how the RAGRepair component creates a repair context from the query, tool execution input and result, and related documents from RAG.</p> <p>For now, the default retrieval method is using sentence embeddings along with Chroma with an option to use BM25 instead. Other storage methods may be considered in the future.</p> <p></p>"},{"location":"concepts/components/rag-repair/#interface","title":"Interface","text":""},{"location":"concepts/components/rag-repair/#input-format","title":"Input format","text":"<p>This component expects documents to be provided in a local path. Optionally, documents can be divided into manual pages/documentation in a <code>man</code> folder and other documents such as troubleshooting documents in a <code>doc</code> folder. If these folders are not provided, then all documents are considered non-documentation. Nested folders are supported for ingesting. Supported files are: <code>pdf</code>, <code>html</code>, <code>json</code>, <code>jsonl</code>.</p> <p>The class <code>post_tool.core.toolkit.RAGRepairRunInput</code> expects three main inputs as follows:</p> <ol> <li> <p><code>messages</code>: List[BaseMessages], a list of messages from the agent, this is optional but will be used to infer the task at hand if <code>nl_query</code> is not provided.</p> </li> <li> <p><code>tool_call</code>: str, this is the tool call generated by the agent.</p> </li> <li> <p><code>nl_query</code>: str (optional), this is the natural language description of the task at hand, the agent should be using this to generate a tool call. Takes priority over <code>messages</code>.</p> </li> <li> <p><code>error</code>: str (optional), this is the response from the tool, expected to describe what the error is.</p> </li> </ol> <p>5<code>original_function</code>: function (optional), this is the original tool that was called, if provided will re-call the tool with the repaired command.</p>"},{"location":"concepts/components/rag-repair/#output-format","title":"Output format","text":"<p>The output is a <code>RAGRepairRunOutput</code> object with the following three properties:</p> <ol> <li> <p><code>new_cmd</code>: str, the repaired tool call.</p> </li> <li> <p><code>retrieved_docs</code>: str, a block of text aggregating all the retrieved documents used.</p> </li> <li> <p><code>result</code>: str, the result of re-calling the original tool with the new command. Will only be filled if <code>original_function</code> is given.</p> </li> </ol>"},{"location":"concepts/components/rag-repair/#results","title":"Results","text":"<p>This component has been evaluated on a set of kubectl commands. The table below details a manual evaluation on how often this component correctly repairs failing kubectl commands.</p> <p>Additional results are reported in [1].</p> <p></p>"},{"location":"concepts/components/rag-repair/#getting-started","title":"Getting Started","text":"<p>Refer to this README for instructions on how to get started with the code.</p> <p>See an example in action here.</p>"},{"location":"concepts/components/rag-repair/#references","title":"References","text":"<p>[1] Tsay, J., et al., \"Repairing Tool Calls Using Post-tool Execution Reflection and RAG,\" arXiv preprint arXiv: (2025).  https://arxiv.org/abs/2510.17874</p>"},{"location":"concepts/components/refraction/","title":"Refraction","text":"<p>Refraction is a low-cost (no LLMs!), low-latency, domain-agnostic, data-agnostic, model-agnostic approach towards validation and repair for a sequence of tool calls, based on classical AI planning techniques.</p> <p>The process of refraction accepts an LLM response and makes appropriate edits to it before passing it off to a downstream applications (in contrast to reflection, where we send back the response for the LLM to reason with again).</p> <p>Compared to the reasoning step, this process must be:</p> <ol> <li>Quick, in relation to the reasoning step itself; and</li> <li>With certain guarantees, since we are passing on the input to downstream applications.</li> </ol> <p>In the context of agentic systems, refraction appears as a middleware component to be executed between the generation and the execution of a tool call, as well as offline to improve model performance based on cached refraction outputs.</p> <p></p>"},{"location":"concepts/components/refraction/#overview","title":"Overview","text":"<p>The refraction API allows an agentic system to validate a tool call or a sequence of tool calls against a reference specification and memory. The whole process works on two simple but key ideas:</p> <p>\ud83d\udca1 A large section of issues with tool calls is already identifiable from the specification of the tool (and an operating memory in the context of a sequential execution of multiple tools) without trying to execute it, or sending it back again to a large model for reflection.</p> <p>\ud83d\udca1 Planning and validating at the sequence level, while not necessary, significantly saves on costs, latency, and on occasions, accuracy of agentic systems rather than doing so one step at a time.</p>"},{"location":"concepts/components/refraction/#architecture","title":"Architecture","text":"<p>To find out if there are any issues with a sequence of tool calls, we cast the debugging task into an optimization process. Consider the following sequence.</p> <pre><code>var2 = SkyScrapperSearchAirport(query=\"London\")\nvar3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", ..., date=\"2024-08-15\")\n</code></pre> <p>We can extract a series of tokens describing the following different aspects of this sequence:</p> <ol> <li>SkyScrapperSearchAirport is called with the parameter <code>query</code>.</li> <li>The parameter <code>query</code> has been assigned to \"London\" which has, presumably, come from the user.</li> <li>The output of the call to SkyScrapperSearchAirport is assigned to <code>var2</code>.</li> <li>SkyScrapperFlightSearch is called with parameters <code>originSkyId</code>, ..., <code>date</code>.</li> <li>The parameter <code>originSkyId</code> is assigned to the key <code>skyId</code> of <code>var1</code> produced previously in the sequence.</li> <li>And so on so forth.</li> </ol> <p>\ud83d\udca1 Given a sequence of API calls, we extract all such tokens. Then the task of the optimizer is: Given the specification of those APIs, enforce as many of the tokens as possible.</p> <p>In order to achieve this, we use your favorite NL2Flow package to check the extracted tokens for soundness i.e. if the tokens represent a valid sequence of API calls. Check the validator API of NL2Flow for more details on this.</p>"},{"location":"concepts/components/refraction/#results","title":"Results","text":"<p>As mentioned before, since this is a validator with guarantees, if there is a structural issue in a sequence of tools calls, it will find it -- subject to [coverage]. Thus, a passing the validation is a necessary condition for the validity of a series of tool calls.</p> <p>Passing the YES call is not sufficient, because the values assigned to the parameters may turn out to be incorrect, in which case the tool call will fail even while being syntactically correct. Furthermore, the fix suggested with a NO signal may not itself be sufficient to make a valid call. This phenomenon is illustrated below.</p> <p></p> <p>Exactly what percentage of calls will go onto the second stage depends on the model and the domain. For example,</p> <ul> <li>One-shot performance of llama-3-70b shows 25/82 success rate for the whole sequence, and 92/226 successful tool calls overall, in the NESTFUL dataset.</li> <li>Above compared to 100/254 incorrect tool calls in a ReWOO/llama-3.1-70b setup, with ReAct mode identifying 20 less issues out of 82 trajectories with less thrashing (compression rate of 0.13 versus 0.15).</li> </ul> <p>For scaling characteristics, see here.</p>"},{"location":"concepts/components/refraction/#getting-started","title":"Getting Started","text":"<ol> <li>Examples and Coverage [link]</li> <li>Details about the refraction API [link]</li> <li>Refraction as a tool decorator [link]</li> <li>Cost-model of refraction as an optimization problem [link]</li> <li>Scaling characteristics [link]</li> <li>Offline API [link]</li> </ol> <p>Refer to this README for instructions on how to get started with the code.</p>"},{"location":"concepts/components/refraction/#references","title":"References","text":"<p>If you end up using this code, you can cite us using the following BibTex entry. Our general focus on natural language processing pipelines for workflow construction applications is documented in that paper.</p> <pre><code>@inproceedings{chakraborti2022natural,\n  title={From Natural Language to Workflows: Towards Emergent Intelligence in Robotic Process Automation},\n  author={Tathagata Chakraborti and Yara Rizk and Vatche Isahagian and Burak Aksar and Francesco Fuggitti},\n  booktitle={BPM RPA Forum},\n  year={2022},\n}\n</code></pre>"},{"location":"concepts/components/silent-review/","title":"Silent Review","text":"<p>A prompt-based component to automatically identify silent errors in tool calls - errors that do not produce explicit error messages.</p> <p>It evaluates whether a tool's response is relevant, accurate, and complete relative to the user\u2019s query.</p>"},{"location":"concepts/components/silent-review/#overview","title":"Overview","text":"<p>Silent Review is designed to be integrated into agent pipelines to assess tool outputs, particularly for:</p> <ul> <li>Verbose outputs: Responses that are long or complex.</li> <li>Tabular outputs: Structured data that requires context-aware evaluation.</li> </ul> <p>The component analyzes the tool response based on:</p> <ol> <li>User query</li> <li>Tool response</li> <li>Tool specification (Optional)</li> <li>Tool input</li> <li>Tool type</li> </ol> <p>It returns one of three outcomes:</p> <ol> <li>Accomplished \u2013 The tool fully satisfies the user query.</li> <li>Partially Accomplished \u2013 The tool partially satisfies the user query.</li> <li>Not Accomplished \u2013 The tool fails to satisfy the query.</li> </ol>"},{"location":"concepts/components/silent-review/#architecture","title":"Architecture","text":"<p>Silent Review works by prompting a large language model (LLM) to evaluate the tool response in the context of the user\u2019s query and the tool\u2019s specification.</p> <p>Integration into an agent pipeline is straightforward:</p> <p></p> <p>Key points: - Works for any structured or JSON-like tool response. - Can be plugged post-tool execution in an agent pipeline. - Uses the LLM to reason about relevance, completeness, and correctness.</p>"},{"location":"concepts/components/silent-review/#results","title":"Results","text":"<p>Silent Review improves the overall reliability of agent tool calls by catching silent errors that would otherwise go unnoticed.</p> <p>Evaluation Dataset</p> <p>The evaluation results are based on the BIRD dataset, which includes SQL databases, natural language questions created by human annotators, and corresponding ground-truth SQL queries. This dataset is designed to evaluate large language models' ability to execute tool calls effectively.</p> <p>For more details, refer to the paper: Invocable APIs derived from NL2SQL datasets for LLM Tool-Calling Evaluation</p> <p>Evaluation Metrics: - Micro Win Rate: Average performance across individual data subsets. - Macro Win Rate: Overall performance across all samples in all subsets.</p> Method Micro Win Rate (%) Micro Avg. Loop Macro Win Rate (%) Macro Avg. Loop React without Review 6.8 8.72 6.1 8.51 React with Review 12.7 7.77 10.4 7.90 <p>Insights: - Adding Silent Review nearly doubles the micro win rate, indicating more queries are fully or partially accomplished. - Average loop counts decrease slightly, showing that fewer iterations are needed to reach successful query completion.</p>"},{"location":"concepts/components/silent-review/#getting-started","title":"Getting Started","text":"<p>Refer to this README for instructions on how to get started with the code. See an example in action here.</p>"},{"location":"concepts/components/sparc/","title":"SPARC","text":"<p>This component evaluates tool calls before execution, identifying potential issues and suggesting corrections or transformations across multiple validation layers.</p>"},{"location":"concepts/components/sparc/#overview","title":"Overview","text":"<p>The Semantic Pre-execution Analysis for Reliable Calls (SPARC) component provides multi-layered validation for tool calls in agentic systems. It combines syntactic validation, semantic analysis, and intelligent parameter transformations to ensure tool calls are correct, appropriate, and properly formatted before execution.</p> <p>This component is designed to be used by any tool-calling agent right before tool execution, allowing you to configure metrics and checks based on your specific use case requirements.</p>"},{"location":"concepts/components/sparc/#key-components","title":"Key Components","text":"<ol> <li>Syntactic Validation: Python-based static analysis of tool call structure (fast and 100% accurate)</li> <li>Semantic Analysis: LLM-as-a-judge evaluation of intent alignment and appropriateness</li> <li>Parameter Transformation: Code generation for complex value transformations (units, formats, etc.)</li> <li>Flexible Configuration: Multiple pre-configured validation profiles for different use cases</li> </ol>"},{"location":"concepts/components/sparc/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                SPARC Reflection Middleware                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Static    \u2502  \u2502   Semantic   \u2502  \u2502   Transformation    \u2502 \u2502\n\u2502  \u2502 Validation  \u2502  \u2502  Analysis    \u2502  \u2502    Validation       \u2502 \u2502\n\u2502  \u2502             \u2502  \u2502              \u2502  \u2502                     \u2502 \u2502\n\u2502  \u2502 (Python)    \u2502  \u2502 (LLM based)  \u2502  \u2502    (LLM based)      \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    LLMEvalKit Integration                   \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502                   Reflection Pipeline                   \u2502 \u2502\n\u2502 \u2502  \u2022 Metrics Engine    \u2022 LLM Provider    \u2022 Result Proc    \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/components/sparc/#input-format","title":"Input Format","text":"<p>SPARC expects three main inputs in OpenAI-compatible formats: - List of messages representing the conversation context - Array of tool specifications following OpenAI function calling format - The tool call generated by your agent that needs validation</p>"},{"location":"concepts/components/sparc/#results","title":"Results","text":""},{"location":"concepts/components/sparc/#tau-bench-airline-domain-evaluation","title":"Tau-bench Airline Domain Evaluation","text":"<p>We evaluated the SPARC Middleware on the Tau-bench airline domain to demonstrate its effectiveness in improving agent performance. In this experiment, we tested the reflection component by adding a reflection step before each tool call. If the tool call was incorrect, the reflection explanation and correction suggestions were returned to the agent to revise the tool call. Otherwise, the tool call was executed as usual.</p> <p>The results below compare the regular agent (without reflection) and the enhanced version with reflection. The reflection includes both syntactic validation and fast track semantic validation (2 general metrics: hallucination detection and agentic constraint satisfaction).</p> <p>The experiment was conducted across multiple agent models - GPT-4o, GPT-4o Mini, and Mistral Large - with various reflection models, including a case where the agent is GPT-4o Mini and the reflector is a stronger model like GPT-4o.</p>"},{"location":"concepts/components/sparc/#metrics-definition","title":"Metrics Definition","text":"<ul> <li>Average Reward (Pass^1): The mean score across all tasks, reflecting overall agent performance</li> <li>Pass^k: K successful task attempts out of all possible attempts (\u2265k)</li> </ul>"},{"location":"concepts/components/sparc/#results-table","title":"Results Table","text":"Model Metric Without Reflection With Reflection Improvement (%) GPT-4o Average Reward 0.47 0.485 +3.19% GPT-4o Pass^1 0.47 0.485 +3.19% GPT-4o Pass^2 0.3567 0.38 +6.54% GPT-4o Pass^3 0.3 0.335 +11.67% GPT-4o Pass^4 0.26 0.3 +15.38% GPT-4o Mini (reflection by GPT-4o) Average Reward 0.175 0.185 +5.71% GPT-4o Mini (reflection by GPT-4o) Pass^1 0.175 0.185 +5.71% GPT-4o Mini (reflection by GPT-4o) Pass^2 0.0833 0.1067 +28.05% GPT-4o Mini (reflection by GPT-4o) Pass^3 0.04 0.085 +112.50% GPT-4o Mini (reflection by GPT-4o) Pass^4 0.02 0.08 +300.00% Mistral Large (35 steps) Average Reward 0.08 0.1 +25.00% Mistral Large (35 steps) Pass^1 0.08 0.1 +25.00% Mistral Large (35 steps) Pass^2 0.0133 0.0333 +150.38% Mistral Large (35 steps) Pass^3 0 0.01 \u2014 Mistral Large (35 steps) Pass^4 0 0 \u2014 Mistral Large (60 steps) Average Reward 0.12 0.13 +8.33% Mistral Large (60 steps) Pass^1 0.12 0.13 +8.33% Mistral Large (60 steps) Pass^2 0.0367 0.0667 +81.84% Mistral Large (60 steps) Pass^3 0.01 0.045 +350.00% Mistral Large (60 steps) Pass^4 0 0.04 \u2014"},{"location":"concepts/components/sparc/#key-findings","title":"Key Findings","text":"<ol> <li> <p>Consistent Improvement in Multi-Attempt Success: The reflection mechanism shows significant improvements in Pass^2, Pass^3, and Pass^4 metrics across all models, indicating better performance when agents have multiple attempts to solve tasks.</p> </li> <li> <p>Model-Dependent Benefits: While GPT-4o shows consistent improvements across all metrics, smaller models like GPT-4o Mini benefit more when reflection is performed by a stronger model (GPT-4o) rather than self-reflection.</p> </li> <li> <p>Substantial Gains for Weaker Models: Mistral Large shows dramatic improvements, particularly in higher-order Pass metrics, demonstrating that reflection is especially valuable for models that initially struggle with tool calling accuracy.</p> </li> </ol>"},{"location":"concepts/components/sparc/#rewardbench-evaluation","title":"RewardBench Evaluation","text":"<p>We also evaluated the middleware on RewardBench, a benchmark designed to evaluate reward model performance in function-calling tasks. The benchmark features 1,500 unique user inputs derived from the single-turn splits of the BFCL-v3 dataset, with each input paired with both correct and incorrect function calls.</p> <p>Configuration: llama-4-maverick model using fast track validation (2 general metrics: hallucination detection and agentic constraint satisfaction)</p> Model TPR TNR Accuracy Time/Sample SPARC Reflector 84.33% 91.87% 88.10% 2.5 seconds Granite-Guardian-3.2-5B 37.33% 20.60% 54.27% 50 milliseconds FORM-1.5B 75.40% 71.07% 73.23% 12 milliseconds FORM-3B 64.27% 86.60% 75.43% 25 milliseconds <p>Important Note: The SPARC Reflector is fundamentally different from traditional reward models. While reward models (Granite-Guardian, FORM-1.5B, FORM-3B) produce continuous scores between 0 and 1, the reflector focuses on:</p> <ul> <li>Error Localization: Identifying specific issues in tool calls</li> <li>Evidence Provision: Explaining why a tool call is problematic</li> <li>Correction Suggestions: Providing actionable recommendations for fixes</li> <li>Configurable Output: Users can control output verbosity (explanation-only, evidence, corrections) to balance detail vs. processing time</li> </ul> <p>This comprehensive analysis requires generating more tokens than simple reward scoring, which explains the longer processing time. However, the reflector's detailed feedback enables agents to actually correct their tool calls rather than just knowing they made an error.</p>"},{"location":"concepts/components/sparc/#bfcl-v3-detection-performance","title":"BFCL-v3 Detection Performance","text":"<p>We evaluated the reflector's ability to distinguish between correct and incorrect tool calls on the BFCL-v3 benchmark. The slow-track reflector (which includes multiple metrics and unit transformation checks) was tested on tool calls generated using Granite Function Calling 20B and compared against ground truth labels.</p>"},{"location":"concepts/components/sparc/#single-turn-performance","title":"Single-Turn Performance","text":"Metric Score Generator Model Granite Function Calling 20b Reflector Model Llama-3-3-70b Accuracy 0.872 Precision 0.823 Recall 0.765 F1 Score 0.793"},{"location":"concepts/components/sparc/#multi-turn-performance","title":"Multi-Turn Performance","text":"Metric Score Generator Model Granite Function Calling 20b Reflector Model Phi-4 Accuracy 0.866 Precision 0.944 Recall 0.905 F1 Score 0.924 <p>These results demonstrate the reflector's strong ability to accurately identify problematic tool calls across both single-turn and multi-turn scenarios. The multi-turn configuration shows particularly high precision (94.4%) and recall (90.5%), indicating excellent performance in complex conversational contexts where tool calls may depend on previous interactions.</p>"},{"location":"concepts/components/sparc/#getting-started","title":"Getting Started","text":"<p>Refer to this README for instructions on how to get started with the code. See an example in action here.</p>"},{"location":"concepts/components/spotlight/","title":"Spotlight","text":"<p>SpotLight enables users to emphasize important spans within their prompt and steers the LLMs attention towards those spans.</p>"},{"location":"concepts/components/spotlight/#overview","title":"Overview","text":"<p>SpotLight is an inference-time lifecycle component that lets you \"highlight\" critical instructions, dynamically steering the model's attention without any retraining. SpotLight only works with locally loaded HuggingFace Transformer models.</p>"},{"location":"concepts/components/spotlight/#architecture","title":"Architecture","text":"<p>Given a user prompt with a critical instruction, this component will boost the attention score of the highlighted instruction. This allows the LLM to follow that specific instruction given its higher attention score.</p>"},{"location":"concepts/components/spotlight/#results","title":"Results","text":"<p>The bar graphs below show how SpotLight leads to improved end to end performance of LLMs.</p> <p></p>"},{"location":"concepts/components/spotlight/#getting-started","title":"Getting Started","text":"<p>Refer to this README for instructions on how to get started with the code. See an example in action here.</p>"},{"location":"concepts/components/spotlight/#references","title":"References","text":"<p>Venkateswaran, Praveen, and Danish Contractor. \"Spotlight Your Instructions: Instruction-following with Dynamic Attention Steering.\" arXiv preprint arXiv:2505.12025 (2025). https://arxiv.org/pdf/2505.12025</p>"},{"location":"concepts/components/summary/","title":"ALTK Components","text":"<p>We summarize the components currently in ALTK in the table below.</p> <p></p> Lifecycle Step Component Problem Description Performance Resources Pre-LLM Spotlight Agent does not follow instructions in the prompt. SpotLight enables users to emphasize important spans within their prompt and steers the LLMs attention towards those spans. It is an inference-time hook and does not involve any training or changes to model weights. 5 and 40 point accuracy improvements Paper Pre-tool Refraction Agent generates inconsistent tool sequences. Verify the syntax of tool call sequences and repair any errors that will result in execution failures. 48% error correction Demo Pre-tool SPARC The agent calls incorrect tools (in the wrong order, redundantly, etc.) or uses incorrect or hallucinated arguments. Evaluates tool calls before execution, identifying potential issues and suggesting corrections with reasoning for tool selection or argument values, including the corrected values. Achieved 88% accuracy in detecting tool-calling mistakes and +15% improvement in end-to-end tool-calling agent pass^k performance across GPT-4o, GPT-4o-mini, and Mistral-Large models. Post-tool JSON Processor Agent gets overwhelmed with large JSON payloads in its context. If the agent calls tools which generate complex JSON objects as responses, this component will use LLM based Python code generation to process those responses and extract relevant information from them. +3 to +50 percentage point gains observed across 15 model from various families and sized on a dataset with 1298 samples Paper, Demo Post-tool Silent Review Tool calls return subtle semantic errors that aren\u2019t handled by the agent. A prompt-based approach to identify silent errors in tool calls (errors that do not produce any visible or explicit error message); Determines whether the tool response is relevant, accurate and complete based on the user's query 4% improvement observed in end-to-end agent accuracy Post-tool RAG Repair Agent isn\u2019t able to recover from tool call failures. Given a failing tool call, this component attempts to use an LLM to repair the call while making use of domain documents such as documentation or troubleshooting examples via RAG. This component will require a set of related documents to ingest 8% improvement observed on models like GPT-4o Paper Pre-Response Policy Guard Agent returns responses that violate policies or instructions. Checks if the agent's output adheres to the policy statement and repairs the output if it does not +10 point improvement in accuracy Paper"},{"location":"concepts/components/test-case-generation/","title":"Test Case Generation Toolkit","text":"<p>This component performs Test Case Generation. It first generates the test case values and then generates an NL Utterance that formulates the test case values into a user query for validating robustness of tools and agents.</p> <p>This toolkit generates testcases for the parameters present in the python tool adhering to data types, data formats and any internal parameter dependencies. This component is designed to perform robust testing of tools.</p>"},{"location":"concepts/components/test-case-generation/#features","title":"Features","text":"<ul> <li>Generates a test case with all mandatory and all optional parameters.</li> <li>Generates a test case with all mandatory parameters.</li> <li>Generates remaining test cases with all mandatory and some optional parameters.</li> <li>Enables user to provide test case values which is then used by the system to generate testcases in the above order.</li> </ul>"},{"location":"concepts/components/test-case-generation/#architecture","title":"Architecture","text":"<p>The figure below shows the flow of test case generation.</p> <p></p>"},{"location":"concepts/components/test-case-generation/#interface","title":"Interface","text":"<p>This component expects the following inputs and generates the following output.</p>"},{"location":"concepts/components/test-case-generation/#input","title":"Input","text":"<ol> <li><code>python_tool_str</code>: Python langchain Tool loaded as string that follows google docstring format.</li> <li><code>test_case_values</code>: The tool parameter values with which test cases will be generated.</li> </ol>"},{"location":"concepts/components/test-case-generation/#output","title":"Output","text":"<p><code>nl_test_cases</code>: Generated test cases with NL utterances in a dictionary format.</p>"},{"location":"concepts/components/test-case-generation/#benchmarking","title":"Benchmarking","text":"<p>Test Case generation is evaluated using LLM (llama-3.1-405b) as a judge and scored the generated utterances on two metrics:</p> <ol> <li>Accuracy (range 1 to 5): Missing or incorrect parameters lead to a reduction in score.</li> <li>Fluency (range 1 to 5): Fluency describes how human-like the NL test case is.</li> </ol> <p>The evaluation was performed on 400 simple APIs from BFCLv3, with an additional 120 human-curated domain-specific API invocations.</p> <p>The distribution of these metrics are as follows:</p> <p> </p>"},{"location":"concepts/components/test-case-generation/#getting-started","title":"Getting Started","text":"<p>Refer to this README for instructions on how to get started with the code. See an example in action here.</p>"},{"location":"concepts/components/tool-enrichment/","title":"Tool Enrichment Component","text":"<p>This component performs python tool enrichment using the metadata information in the tool definition for improved tool calling and tool input formation.</p> <p>This component enriches python tool docstrings using the metadata information in the tool definition. This toolkit is designed to enhance the tool docstrings for improved tool learning - i.e., tool selection and tool calling by the LLM.</p>"},{"location":"concepts/components/tool-enrichment/#features","title":"Features","text":"<p>This component accepts as input the tool definition python file as string (consisting of @tool decorator) and outputs the enriched python tool string with modified docstring which consists of the following enrichments: - Enriched tool description using metadata like tool signature, tool method body, current tool description, etc - Improved tool parameter descriptions using metadata like existing parameter descriptions, declarations of global variables and class definitions and other metadata. - New examples corresponding to each parameter of the tool based on parameter descriptions, tool body details, etc. - Enriched tool return descriptions using existing return descriptions, tool body, global variable declarations and other metadata in the tool file.</p>"},{"location":"concepts/components/tool-enrichment/#architecture","title":"Architecture","text":"<p>The figure below shows the flow of tool enrichment.</p> <p></p>"},{"location":"concepts/components/tool-enrichment/#interface","title":"Interface","text":"<p>This component expects the following input and generates the following output.</p>"},{"location":"concepts/components/tool-enrichment/#input","title":"Input","text":"<ol> <li><code>python_tool_str</code>: Python langchain Tool loaded as string that follows google docstring format.</li> </ol>"},{"location":"concepts/components/tool-enrichment/#output","title":"Output","text":"<p><code>enriched_python_tool_str</code>: Python langchain Tool in string format with enriched docstring that follows google docstring format.</p>"},{"location":"concepts/components/tool-enrichment/#benchmarking","title":"Benchmarking","text":"<p>Tool Enrichment is extensively benchmarked on WxO tools. Following is the benchmarking results on ServiceNow tools. When tool descriptions are of poor quality (see tool_name as desc), enrichment increases the number of correct agent and tool calls (~30 pts).</p> <p></p>"},{"location":"concepts/components/tool-enrichment/#getting-started","title":"Getting Started","text":"<p>Refer to this README for instructions on how to get started with the code. See an example in action here.</p>"},{"location":"concepts/components/refraction/00.-Refraction-Home/","title":"Refraction Home","text":"<p>Refraction is a low-cost (no LLMs!), low-latency, domain-agnostic, data-agnostic, model-agnostic approach towards validation and repair for a sequence of tool calls, based on classical AI planning techniques.</p> <p>The process of refraction accepts an LLM response and makes appropriate edits to it before passing it off to a downstream applications (in contrast to reflection, where we send back the response for the LLM to reason with again).</p> <p>Compared to the reasoning step, this process must be:</p> <ol> <li>Quick, in relation to the reasoning step itself; and</li> <li>With certain guarantees, since we are passing on the input to downstream applications.</li> </ol> <p>In the context of agentic systems, refraction appears as a middleware component to be executed between the generation and the execution of a tool call, as well as offline to improve model performance based on cached refraction outputs.</p> <p></p>"},{"location":"concepts/components/refraction/00.-Refraction-Home/#pre-call-and-sequence-level-verification","title":"Pre-call and sequence-level verification","text":"<p>The refraction API allows an agentic system to validate a tool call or a sequence of tool calls against a reference specification and memory. The whole process works on two simple but key ideas:</p> <p>\ud83d\udca1 A large section of issues with tool calls is already identifiable from the specification of the tool (and an operating memory in the context of a sequential execution of multiple tools) without trying to execute it, or sending it back again to a large model for reflection.</p> <p>\ud83d\udca1 Planning and validating at the sequence level, while not necessary, significantly saves on costs, latency, and on occasions, accuracy of agentic systems rather than doing so one step at a time.</p>"},{"location":"concepts/components/refraction/00.-Refraction-Home/#debugging-as-an-optimization-process","title":"Debugging as an optimization process","text":"<p>To find out if there are any issues with a sequence of tool calls, we cast the debugging task into an optimization process. Consider the following sequence.</p> <pre><code>var2 = SkyScrapperSearchAirport(query=\"London\")\nvar3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", ..., date=\"2024-08-15\")\n</code></pre> <p>We can extract a series of tokens describing the following different aspects of this sequence:</p> <ol> <li>SkyScrapperSearchAirport is called with the parameter <code>query</code>.</li> <li>The parameter <code>query</code> has been assigned to \"London\" which has, presumably, come from the user.</li> <li>The output of the call to SkyScrapperSearchAirport is assigned to <code>var2</code>.</li> <li>SkyScrapperFlightSearch is called with parameters <code>originSkyId</code>, ..., <code>date</code>.</li> <li>The parameter <code>originSkyId</code> is assigned to the key <code>skyId</code> of <code>var1</code> produced previously in the sequence.</li> <li>And so on so forth.</li> </ol> <p>\ud83d\udca1 Given a sequence of API calls, we extract all such tokens. Then the task of the optimizer is: Given the specification of those APIs, enforce as many of the tokens as possible.</p> <p>In order to achieve this, we use your favorite NL2Flow package to check the extracted tokens for soundness i.e. if the tokens represent a valid sequence of API calls. Check the validator API of NL2Flow for more details on this.</p>"},{"location":"concepts/components/refraction/00.-Refraction-Home/#example","title":"Example","text":"<p>Here is an example use case from the NESTFUL dataset:</p> <p>User: Find flights from New York to London that depart on August 15, 2024, and return on August 18, 2024 and find hotels in London.</p> <pre><code>var1 = SkyScrapperSearchAirport(query=\"New York\")\nvar2 = SkyScrapperSearchAirport(query=\"London\")\nvar3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\nvar4 = TripadvisorSearchLocation(query=\"London\")\nvar5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre> <p>We have introduced several errors in the previous example before making the call above. This includes, wrong parameters, wrong assignment to parameters, wrong api names, missing variables, etc.</p> <pre><code>from data.data_handler import get_nestful_catalog\nfrom refraction import refract\n\ncatalog = get_nestful_catalog(executable=True)\n\nsequence = [\n    'var1 = SkyCrapperSearchAirport(query=\"New York\")',\n    'var2 = SkyScrapperSearchAirport(query=\"London\")',\n    'var3 = SkyScrapperFlightSearch(originalSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\")',\n    'var45 = TripadvisorSearchLocation(query=\"London\")',\n    'var5 = TripadvisorSearchHotels(geoId=\"$var4.ID$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")',\n]\n\nrefract(sequence, catalog)\n</code></pre> <p>Here is a pretty print of a sample response from the refraction process (it also provides a structured response) on the example above. Notice how different edits have been made to the input sequence. This includes fixes to parameter names, parameter assignments, API names, output variables, missing function calls, and so on.</p> <pre><code>+ var7 = NewsAPISearchByKeyWord()\n- var1 = SkyCrapperSearchAirport(query=\"New York\")\n?    ^      ^\n\n+ var2 = SkyScrapperSearchAirport(query=\"New York\")\n?    ^      ^^\n\n- var3 = SkyScrapperFlightSearch(originalSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\")\n?                                      --           ^                                                               ^\n\n+ var3 = SkyScrapperFlightSearch(originSkyId=\"$var2.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var2.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"$var7.date$\")\n?                                                 ^                                                               ^                                                  ++++++++++++++++++++\n\n  var45 = TripadvisorSearchLocation(query=\"London\")\n- var5 = TripadvisorSearchHotels(geoId=\"$var4.ID$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n?                                              ^\n\n+ var5 = TripadvisorSearchHotels(geoId=\"$var45.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n?                                            + +++ ^\n</code></pre>"},{"location":"concepts/components/refraction/00.-Refraction-Home/#next-steps","title":"Next Steps","text":"<ol> <li> <p>Continue reading on to the next chapter to find out more on the categories of errors handled by this process and the different recovery patterns for different types of errors.</p> </li> <li> <p>Head over to the refraction API on detailed usage instructions.</p> </li> </ol>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/","title":"Refraction Examples","text":"<p>We started by saying that one of the key ideas for refraction is that a large section of issues with tool calls is already identifiable from the specification of the tool (and an operating memory in the context of a sequential execution of multiple tools) without trying to execute it, or sending it back again to a large model for reflection.</p> <p>This means that there are certain things it can do, and there are many things it cannot. Depending on your needs, you can make a decision based on the following.</p>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#what-it-does","title":"What it does","text":"<p>If it is in the input spec, we will try to cover it -- we are in the process of implementing full coverage. This means common issues with parameters, memory references, tool names, etc. as we detail below.</p> <ul> <li>[x] Made up API</li> <li>[x] Wrong label</li> <li>[x] Missing label</li> <li>[x] Missing input parameter</li> <li>[x] Made up input parameter</li> <li>[x] Made up input parameter assignment to variable</li> <li>[x] Made up input parameter assignment to property</li> <li>[x] Made up assignment recovery with ask</li> <li>[x] Made up assignment recovery with mapping</li> <li>[x] Made up assignment recovery with function call</li> <li>[x] Missing step</li> <li>[ ] Type mismatch</li> <li>[ ] Inadmissible assignments to enum inputs</li> <li>[x] Optional items in the parameters</li> <li>[x] Ordering of parameters</li> <li>[ ] Transformation of parameters</li> </ul>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#what-it-doesnt-do","title":"What it doesn't do","text":"<p>This also means that we DO NOT at all consider issues with the values assigned to parameters. In other words, a mistaken tool call with an issue with, for example, the format of a date string assigned to a date input, is out of scope. Only an LLM can fix those issues through a traditional reflection approach.</p>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#11-categories-of-errors","title":"1.1 Categories of errors \ud83d\udc1e","text":"<p>In this page, we will cover some common mistakes in a sequence of API calls (generated from an LLM) and the result of refraction on each of them, in isolation. The cost model of the debugging API determines the response when many such errors can encountered simultaneously.</p> <p>We will start with the following sequence of API calls and make modifications to illustrate the refraction process.</p> <pre><code>var1 = SkyScrapperSearchAirport(query=\"New York\")\nvar2 = SkyScrapperSearchAirport(query=\"London\")\nvar3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\nvar4 = TripadvisorSearchLocation(query=\"London\")\nvar5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#messed-up-api-function-name","title":"Messed up API / function name","text":"<p>In this example, the API or function call has been misspelled \ud83d\udc26. The refractor corrects it to the correct name. Note that this is NOT done by computing the closest valid API or function name. As explained before, the debugging process is one of optimizing the set of valid tokens. Here, given all the parameters mentioned, the least costly edit turns out to be this API that can make use of the other tokens around it. As a result, this fix is agnostic to how messed up the wrong token was.</p> <pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n- var3 = SkyCrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n  var4 = TripadvisorSearchLocation(query=\"London\")\n+ var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre> <p>Note that the fixed sequence is not exactly the same as the starting sequence -- it is in a different order. This is because the API calls mentioned in it can occur in a couple of equivalent orders. Executing this sequence will produce the same outcome as the original one.</p>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#wrong-output-label","title":"Wrong output label","text":"<p>Now we have messed up the output label <code>var2</code> to <code>var20</code>. This messes up the following steps as well. For example, the assignment of <code>destinationSkyId</code> to <code>$var2.skyId$</code> is inadmissible now. The debugger has injected an extra line with the correct output label as a result. Again, from the optimization point of view, this edit is not syntactic but is made because this single edit is enough to make all the other tokens valid.</p> <pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var20 = SkyScrapperSearchAirport(query=\"London\")\n+ var2 = SkyScrapperSearchAirport(query=\"London\")\n  var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre> <p>Notice that while introducing the new step, the debugger has also kept the old vestigial step. It's hanging around there like an appendicitis. This is because we are interpreting a token as \"The output of <code>SkyScrapperSearchAirport</code> was assigned to <code>var20</code>\" as one to enforce. We may look into ignoring this token in the future.</p>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#missing-output-label","title":"Missing output label","text":"<p>Instead of a wrong label, here we have removed the label altogether. This has the same impact (inadmissible assignments) on the remaining sequence as in the previous case. The debugger restores the missing token.</p> <pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n- SkyScrapperSearchAirport(query=\"London\")\n+ var2 = SkyScrapperSearchAirport(query=\"London\")\n? +++++++\n\n  var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#missing-input-parameter","title":"Missing input parameter","text":"<p>Now we have removed an input (required) parameter from the API call. Notice that the debugger has put it back. Notice that it has also introduced a slot-fill for it, instead of the assignment in the original sequence.</p> <p>More about such recovery patterns later.</p> <pre><code>+ ask(originSkyId)\n  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n- var3 = SkyScrapperFlightSearch(destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n+ var3 = SkyScrapperFlightSearch(originSkyId=originSkyId, destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                +++++++++++++++++++++++++\n\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#made-up-input-parameter","title":"Made up input parameter","text":"<p>Similar to the previous example, now we have messed up the name of the parameter instead removing it altogether. The effect is the same. Note that, as discussed previously, this is not an edit based on the text of the mistake but rather on the tokens required by the underlying API specs. That is why a missing parameter and a made-up parameter are almost identical in practice (with certain caveats in recovery patterns).</p> <pre><code>+ ask(originSkyId)\n  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n- var3 = SkyScrapperFlightSearch(originalSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                      --      ^^^^ ^^^    --\n\n+ var3 = SkyScrapperFlightSearch(originSkyId=originSkyId, destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                            ^ ^^^^^\n\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#assignment-to-made-up-variable","title":"Assignment to made up variable","text":"<p>Now we have messed up the variable in the parameter assignment, by changing <code>var2</code> to a non-existent <code>var20</code>. The debugger puts it back.</p> <pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n- var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var20.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                                                                   -\n\n+ var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#assignment-to-made-up-variable-parameter","title":"Assignment to made up variable parameter","text":"<p>Now, instead of the variable name being wrong, the assignment of the parameter is being made to an invalid field i.e. the output object <code>var2</code> does not contain that field. The debugger has identified the issue and recommended a slot-fill for the missing value. It could have also mapped to an existing key -- we will study this pattern in more detail below among the recovery patterns.</p> <pre><code>+ ask(skyayeId)\n  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n- var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyayeId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                                                             -------        --\n\n+ var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=skyayeId, originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#missing-step","title":"Missing step","text":"<p>Finally, we have removed an entire step from the original sequence here. The debugger has suggested a fix. It happens to be perfect here, but there is a lot of intrigue to what this fix may be in practice. There are many things up in the air here:</p> <ol> <li> <p>The debugger does not know that something is missing in the first place. Recall that the debugger here has no higher level abstract goal other than to enforce whatever it is given. So whether a new step will be suggested at all is dependent on whether there are unrequited tokens in the remaining sequence (in this case, <code>$var4.geoId$</code>) and additionally from the optimization point of view, whether it is cheaper to just edit out the unrequited tokens instead of introducing a new one.</p> </li> <li> <p>Furthermore, what the assignment of the parameter in the new step is, is also up for debate. It could have been \"New York\" (50-50 chance). While it might be possible to improve upon random assignments in the future, In such situations it is advisable to use defensive recovery mechanisms discussed below.</p> </li> </ol> <pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n+ var4 = TripadvisorSearchLocation(query=\"London\")\n  var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#12-recovery-patterns","title":"1.2 Recovery Patterns","text":"<p>So far, we have been concentrating on the individual mistakes made in a sequence of API / function calls. Now we will look at the suggestions recommended by the debugger in a bit more detail.</p>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#121-self-contained","title":"1.2.1 Self-contained","text":"<p>The minimal case is when all the information to fix one or more problematic tokens is there in the input sequence itself. Then the recovery pattern is confined to an in-situ fix. This is the case with the first 3 examples above.</p>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#122-slot-filling","title":"1.2.2 Slot-filling","text":"<p>A slot-filling response is when there is an issue with assigning a value to a parameter based on the available tokens and so the debugger suggests we ask its value from the user. This is the case in the next 2 examples. Ideally, we want to reuse the existing tokens, so a slot-fill response is a less preferred recovery compared to maps and function calls.</p> <pre><code>+ ask(originSkyId)\n  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n- var3 = SkyScrapperFlightSearch(destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n+ var3 = SkyScrapperFlightSearch(originSkyId=originSkyId, destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                +++++++++++++++++++++++++\n\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#123-mapping","title":"1.2.3 Mapping","text":"<p>A mapping is an assignment of a parameter to a value (e.g. <code>query=\"London\"</code>) or to another variable (e.g. <code>destinationSkyId=\"$var20.skyId$\"</code>). A fix involving a mapping can appear in two different forms.</p>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#reusing-existing-maps","title":"Reusing existing maps","text":"<p>Let's go back to the example of an assignment to a made up variable. Here the fix to an invalid assignment has been done based on assignments already available from the existing tokens.</p> <p>Specifically, <code>var2</code> already exists and the original tokens showed that the parameter <code>destinationSkyId</code> can be assigned to <code>skyId</code> -- so all the debugger needs to do is to produce a <code>skyId</code> from a call to <code>SkyScrapperSearchAirport</code> (as promised by its API spec) and then assign the output of that call to <code>var2</code>. \ud83d\ude0c</p> <pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n- var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var20.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                                                                   -\n\n+ var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#using-a-new-map","title":"Using a new map","text":"<p>We can also ask the debugger to think of new maps if possible, and not ask the user unless possible. This is an optional feature and has impact on performance.</p> <p>Compare the results with that in the same situation in a previous example.</p> <pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n- var3 = SkyScrapperFlightSearch(originalSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                      --           ^\n\n+ var3 = SkyScrapperFlightSearch(originSkyId=\"$var2.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                                 ^\n\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre> <p>As noted previously, currently the debugger picks a valid map at random so this map could have (and should have) gone to the parameter <code>skyId</code> from <code>var1</code> as well. This might be possible to improve upon.</p> <p>This fix is not entirely syntactic. The newly introduced maps are ones that are close to the unrequited variable in some text embedding space (computed offline for a given catalog of APIs).</p>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#124-function-calls","title":"1.2.4 Function Calls","text":"<p>Introducing a new map may also involved introducing a new step with an new function call. This recovery pattern can appear in two different contexts.</p>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#missing-step_1","title":"Missing step","text":"<p>We saw this example in the example above with a missing step. As we discussed previously, the debugger does not know that a step is missing. So whether a new step is introduced depends on whether introducing new things are preferred over deleting already existing stuff per the cost model being used.</p> <pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n+ var4 = TripadvisorSearchLocation(query=\"London\")\n  var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#enabler-of-a-new-mapping","title":"Enabler of a new mapping","text":"<p>A new call is not be confined to missing steps only. If a invalid token can be possibly fixed with a different API, e.g. by producing a new variable that can be mapped to an existing variable whose assignment token needs to be satisfied, then the debugger will introduce a new step if, as in the previous case, introducing new things are preferred over deleting already existing stuff per the cost model being used.</p> <p>We have now removed the (required) date parameter and the debugger has introduced a call to acquire the current date. This invocation may be dangerous if unsupervised at runtime. This brings us next to the topic of defensive recovery.</p> <pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n+ var7 = NewsAPISearchByKeyWord()\n- var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\")\n+ var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"$var7.date$\")\n?                                                                                                                                                                    ++++++++++++++++++++\n\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#125-defensive-recovery","title":"1.2.5 Defensive Recovery","text":"<p>In systems that execute on the results of their reasoning, it is advisable to use \"defensive execution\" where the system can confirm certain aspects of its reasoning (in this context, the component of an API call) before acting on it. The NL2Flow package offers some off-the-shelf options for defensive execution such as confirming newly introduced maps, newly determined variables, and so on, as described here.</p> <p>We can re-use the same for debugging, as shown below. Note that, unlike defensive execution in NL2Flow, here the defensive operations will only show up for extra stuff that the debugger has added in.</p> <p>This feature is optional and has impact on performance.</p> <pre><code>refract(tokens, catalog, defensive=True)\n</code></pre> <p>Previously, we saw how newly introduced maps may pick the wrong mapping when picking from available maps at random. We also saw how a new operator may be introduced that the user may not like. Let's revisit the same situations with the defensive guardrails on.</p>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#defense-against-new-mapping","title":"Defense against new mapping","text":"<p>Contrast with the corresponding example above.</p> <pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n+ confirm(originSkyId=\"$var1.skyId$\")\n- var3 = SkyScrapperFlightSearch(destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n+ var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                ++++++++++++++++++++++++++++\n\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/01.-Refraction-Examples/#defense-against-using-information-from-new-operation","title":"Defense against using information from new operation","text":"<p>Contrast with the corresponding example above.</p> <pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n+ var7 = NewsAPISearchByKeyWord()\n+ confirm(date=\"$var7.date$\")\n- var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\")\n+ var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"$var7.date$\")\n?                                                                                                                                                                    ++++++++++++++++++++\n\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/","title":"The Refraction API | Inputs and Outputs","text":"<p>You can use the refraction API to fix tool calls. As an input, you need to provide, at a minimum, the tool call and the backing tool specs (catalog).</p> <pre><code>from refraction import refract\nresult = refract(&lt;TOOL CALLS&gt;, &lt;CATALOG&gt;)\n</code></pre>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/#21-accepted-inputs","title":"2.1 Accepted Inputs","text":"<p>The form of acceptable tool calls and catalogs follow a wider range of formats, including the NESTFUL form and common tool calling patterns using for calling LLMs. In the following we discuss the raw forms of inputs and outputs. In the next chapter, we will discuss how to use the refraction decorator to annotate tools directly.</p>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/#211-on-a-single-call-using-the-nestful-sequencestep-object","title":"2.1.1 On a single call using the NESTFUL SequenceStep Object","text":"<pre><code>from nestful import SequenceStep\nfrom refraction import refract\n\ncall = {\n    \"name\": \"Tripadvisor_Get_Restaurant_Details\",\n    \"arguments\": {\"restaurantsId\": \"$var2.restaurantsId$\"},\n    \"label\": \"var1\",\n}\n\nresult = refract(\n    sequence=SequenceStep(**call),\n    catalog=catalog,\n)\n</code></pre> <p>The catalog input follows the NESTFUL schema. Check here for an example of what the schema looks like. You can fetch a catalog like so:</p> <pre><code>from nestful.data_handlers import (\n    DataID,\n    get_nestful_catalog,\n    get_nestful_data,\n)\n\ncatalog = get_nestful_catalog(name=DataID.GLAIVE, executable=False)\nsequences, catalog = get_nestful_data(name=DataID.COMPLEXFUNCBENCH)\n</code></pre>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/#212-on-a-sequence-of-calls-using-the-nestful-sequencingdata-object","title":"2.1.2 On a sequence of calls using the NESTFUL SequencingData Object","text":"<p>You can refract on an entire sequence together (this is extra helpful because you can validate the flow of data across multiple tool calls).</p> <pre><code>from nestful import SequencingData\nfrom refraction import refract\n\ncalls = {\n    \"input\": \"Find flights from New York to London that ...\",\n    \"output\": [\n        {\n            \"name\": \"TripadvisorSearchLocation\",\n            \"arguments\": {\"query\": \"London\"},\n            \"label\": \"var1\",\n        },\n        {\n            \"name\": \"TripadvisorSearchHotels\",\n            \"arguments\": {\n                \"geoId\": \"$var2.ID$\",\n                \"checkIn\": \"2024-08-15\",\n                \"checkOut\": \"2024-08-18\",\n            },\n            \"label\": \"var2\",\n        }\n    ]\n}\n\nresult = refract(\n    sequence=SequencingData(**calls),\n    catalog=catalog,\n)\n</code></pre> <p>If you do not want to consider other APIs in the catalog mentioned in the call(s) while coming up with a fix, then use the following input. This will make validation considerably faster.</p> <pre><code>refract(..., use_given_operators_only=True)\n</code></pre> <p>You can also provide a timeout like so:</p> <pre><code>refract(..., timeout=5)\n</code></pre>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/#213-using-a-formatted-string-form","title":"2.1.3 Using a Formatted String Form","text":"<p>You can also use the prettified string form as input directly.</p> <pre><code>from refraction import refract\n\nresult = refract(\n    sequence=[\n        'var1 = TripadvisorSearchLocation(query=\"London\")',\n        'var2 = TripadvisorSearchHotels(geoId=\"$var1.ID$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")',\n    ],\n    catalog=catalog,\n)\n</code></pre>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/#214-using-openai-langchain-tool-calling-formats","title":"2.1.4 Using OpenAI / LangChain tool calling formats","text":"<p>You can also use the format of tool calling used in the OpenAI and LangChain tool calling APIs. These are very commonly used in LLM prompts. Check here for an example.</p> <p>Check here for instructions on refraction while calling tools directly.</p>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/#using-a-mapper","title":"Using a mapper","text":"<p>You can provide a list of mappings (between output and input fields of APIs in the catalog) to help the refractor compute better fixes when the tool calls are corrupted. Mappings should be precomputed and passed in as inputs at runtume.</p> <p>You can compute a list of mappings from a catalog like this (the <code>threshold</code> parameter determines how strong a mapping needs to be at a minimum, and the <code>top_k</code> parameter indicates how many maps for the same field to keep around):</p> <pre><code>from nestful.data_handlers import get_nestful_data\nfrom refraction.mappings.compute_maps import Mapper\n\nsequence, catalog = get_nestful_data(executable=True)\nmapper = Mapper()\n\ncomputed_mappings = mapper.compute_maps(catalog, top_k=1, threshold=0.80)\n</code></pre> <p>Optionally, you can also cache (and merge with existing) the mappings used in already existing sequences like this:</p> <pre><code>from refraction.mappings.utils import cache_maps, merge_maps\n\ncached_mappings = cache_maps(sequence_data)\ncached_mappings.extend(computed_mappings)\n\nmerged_mappings = merge_maps(cached_mappings)\n</code></pre> <p>You need to pass on the mappings into the call like this:</p> <pre><code>from refraction.schemas import Mapping\nfrom refraction import refract\n\nresult = refract(\n    sequence=...,\n    catalog=...,\n    mappings=[\n         Mapping(source_name=\"skyId\", target_name=\"destinationSkyId\"),\n    ],\n)\n</code></pre> <p>Or using the refractor class, initialize this once. This will automatically compute potential maps between APIs in the catalog.</p> <pre><code>from refraction.integration import Refractor\n\nrefractor = Refractor(catalog=catalog)\nrefractor.initialize_maps()\n\nrefractor.refract(...)\n</code></pre> <p>\ud83d\udc49 If you are integrating with a long running agent, and you are not validating a call in isolation, this is the recommended way to use the API.</p>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/#using-a-memory","title":"Using a memory","text":"<p>You can also use a memory if you are executing a sequence but validating tool calls one step at a time. In the sequence example above, the memory would look something like this after executing the first step:</p> <pre><code>from refraction import refract\n\nmemory = {\n    \"var1\": {\n        \"geoId\": \"...\",\n        ...\n    }\n}\n\nresult = refract(\n    sequence=['var2 = TripadvisorSearchHotels(geoId=\"$var1.ID$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")'],\n    catalog=catalog,\n    memory=memory,\n)\n</code></pre> <p>\ud83d\udc40 Note the unresolved reference in the actual call. If you want validation of data flow to be enforced by the refractor, then you must produce calls without yet resolving the references.</p>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/#22-output-format","title":"2.2 Output Format","text":"<p>The output of refraction is an object that contains a determination of whether the tool call is schematically correct or not, a corrected call if possible, etc. The output also contains a prettified diff string of what went wrong.</p>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/#221-result-object","title":"2.2.1 Result Object","text":"<p>Explore the result object to find out details about what happened during the refraction process. You can find out if it was successful by looking up:</p> <pre><code>result.report.determination\n</code></pre> <p>If this is True it means that the call is executable as is. If it is False, it is not; and you can look up the other parts of the result to see what went wrong.</p> <p>If it is None, it means the refraction process failed for some reason, without a definitive determination (the result object will contain information about errors and timeouts).</p> <p>Previously, we mentioned the need for guarantees. We also spoke about how the refractor does not look at the values assigned to parameters. This bears out here in the sense that if the refractor says NO then something is definitely wrong and there is no point trying to execute this call. If it says YES then the call is executable but might be wrong in the values assigned to items.</p>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/#222-prettified-form","title":"2.2.2 Prettified Form","text":"<p>In case the determination is False, you can print out the prettified form for debugging, feedback, or whatever other reason, by doing this:</p> <pre><code>print(\"\\n\".join(result.diff))\n</code></pre> <pre><code>  var45 = TripadvisorSearchLocation(query=\"London\")\n- var5 = TripadvisorSearchHotels(geoId=\"$var4.ID$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n?                                              ^\n\n+ var5 = TripadvisorSearchHotels(geoId=\"$var45.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n?                                            + +++ ^\n</code></pre>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/#223-corrected-call-generation","title":"2.2.3 Corrected Call Generation","text":"<p>If the determination was False but is fixable from within what was provided, you can generate the new corrected call like so:</p> <pre><code>corrected_call = result.corrected_function_call(memory, catalog)\n</code></pre> <p>You can check if this is readily executable (e.g. does not require extra slot-fills for missing parameters) by checking this:</p> <pre><code>cfc.is_executable\n</code></pre>"},{"location":"concepts/components/refraction/02.-The-Refraction-API-%7C-Inputs-and-Outputs/#224-prompt-generation","title":"2.2.4 Prompt Generation","text":"<p>Finally, in case you want to pass on the output of refraction to a help a traditional reflection component, you can do so as follows. This results in a two stage integration in an agentic flow, as illustrated above. In the first stage, if the refractor says YES, then you can chose to attempt an execution. If it says NO, and the corrected function call as described above is also not executable, then you can pass on the feedback to a traditional reflection component.</p> <pre><code>from refraction import refract, generate_prompt\n\nprompt = generate_prompt(\n    result,\n    sequence,\n    catalog,\n    memory_objects=memory,\n    prompt_type=PromptType.WITH_SUGGESTIONS,\n)\n</code></pre> <p>Here is a sample generated prompt from the refraction result to pass on to an LLM.</p> <pre><code>Please fix the provided tool call based on the issues outlined.\n\n&lt;tool_call&gt;[\n{'name': 'TripadvisorSearchHotels', 'arguments': {'location': '$var4.geoId', 'checkIn': '2024-08-15', 'checkOut': '2024-08-18'}, 'label': 'var3'}\n]&lt;/tool_call&gt;\n\nThe following are the identified issues:\nEach issue is accompanied by guidance on how to fix it.\nConsider the guidance, along with the provided tool specs, and memory, to come up with the final fixed tool call.\n\n- Parameter location is not a recognized parameter for the tool TripadvisorSearchHotels.\n- Parameter geoId is a required parameter for TripadvisorSearchHotels, but it is missing.\n- Possible fix: Get value of parameter geoId by calling TripadvisorSearchLocation.\n- Possible fix: Call TripadvisorSearchLocation with parameters: query.\n- Possible fix: Get value of query from the user input: Book a hotel in Boston.\n</code></pre>"},{"location":"concepts/components/refraction/03.-The-Refraction-API-%7C-Tool-Calling/","title":"The Refraction API | Tool Calling","text":"<p>In the previous chapter, we discussed the basic form of the refraction API, where you have the catalog specs, and the generated tool call(s), in structured forms and you want to validate, and act upon the results of the validation, within an agentic system.</p> <p>This chapter will describe a less invasive form where you can annotate tools directly for this purpose.</p>"},{"location":"concepts/components/refraction/03.-The-Refraction-API-%7C-Tool-Calling/#31-decorating-a-tool","title":"3.1 Decorating a Tool","text":"<p>Let's start with tool / function calls outside of existing agentic frameworks. Consider the following tools.</p> <pre><code>from refraction.integration import refract\n\n@refract()\ndef SkyScrapperFlightSearch(\n    originSkyId: str,\n    destinationSkyId: str,\n    originEntityId: str,\n    destinationEntityId: str,\n    date: str,\n    returnDate: Optional[str] = None,\n    cabinClass: Optional[str] = \"economy\",\n    adults: Optional[int] = 1,\n    ...\n) -&gt; Dict[str, Any]:\n    return {...}\n\n\n@refract()\ndef TripadvisorSearchLocation(\n    query: str,\n) -&gt; Dict[str, Any]:\n    return {...}\n\n\n@refract(\n    api=\"TripadvisorSearchHotels\",\n    use_given_operators_only=False,\n    execute_if_fixed=True,\n)\ndef search_hotels(**kwargs: Any) -&gt; Dict[str, Any]:\n    return {...}\n</code></pre> <p>A couple of things to notice here. The tools or function can be in two forms: either the signature itself contains what all it needs (if it wraps an API inside, then these would be the input specification of that API). This is not always possible, and so the refract decorator allows you to annotate with the name of the API it is wrapping instead.</p>"},{"location":"concepts/components/refraction/03.-The-Refraction-API-%7C-Tool-Calling/#311-successful-call","title":"3.1.1 Successful Call","text":"<p>This is a successful call (notice the extra inputs) and the function will execute as if nothing extra has happened!</p> <pre><code>flight_details = SkyScrapperFlightSearch(\n    originSkyId=\"BOS\",\n    destinationSkyId=\"JFK\",\n    originEntityId=\"123\",\n    destinationEntityId=\"456\",\n    date=\"2024-05-09\",\n    # NOTE: Extra parameter\n    refractor=refractor,\n)\n</code></pre>"},{"location":"concepts/components/refraction/03.-The-Refraction-API-%7C-Tool-Calling/#312-faulty-call-with-return","title":"3.1.2 Faulty Call with Return","text":"<p>Now we have missed a parameter in the call.</p> <pre><code>flight_details = SkyScrapperFlightSearch(\n    # missing parameter\n    # originSkyId=\"BOS\",\n    destinationSkyId=\"JFK\",\n    originEntityId=\"123\",\n    destinationEntityId=\"456\",\n    date=\"2024-05-09\",\n    # NOTE: Extra parameters\n    refractor=self.refractor,\n)\n</code></pre> <p>The decorator will block the call and return a refraction result instead. As described in the previous chapter, you can use this to decide how to fix your call.</p> <pre><code>+ ask(originSkyId)\n- var1 = SkyScrapperFlightSearch(destinationSkyId=\"JFK\", originEntityId=\"123\", destinationEntityId=\"456\", date=\"2024-05-09\")\n+ var1 = SkyScrapperFlightSearch(destinationSkyId=\"JFK\", originEntityId=\"123\", destinationEntityId=\"456\", date=\"2024-05-09\", originSkyId=\"$originSkyId$\")\n?                                                                                                                          +++++++++++++++++++++++++++++\n</code></pre>"},{"location":"concepts/components/refraction/03.-The-Refraction-API-%7C-Tool-Calling/#313-faulty-call-with-execution","title":"3.1.3 Faulty Call with Execution","text":"<p>If this call is fixable as is, then setting the following parameter, lets the fixed call to execute in place without returning back to the user, even if the original function call was incomplete / incorrect.</p> <pre><code>use_given_operators_only=False\n</code></pre> <pre><code>memory = {\"query\": \"London\", \"var1\": {\"geoId\": \"foo\"}}\n\npayload = {\n    \"checkIn\": \"2024-09-05\",\n    \"checkOut\": \"2024-09-15\",\n}\n\nresult = search_hotels(\n    **payload,\n    refractor=refractor,\n    memory=memory,\n)\n</code></pre> <pre><code>- var2 = TripadvisorSearchHotels(checkIn=\"2024-09-05\", checkOut=\"2024-09-15\")\n+ var2 = TripadvisorSearchHotels(checkIn=\"2024-09-05\", checkOut=\"2024-09-15\", geoId=\"$var1.geoId$\")\n?                                                                           ++++++++++++++++++++++\n</code></pre> <pre><code>Executing: TripadvisorSearchHotels(checkIn=\"2024-09-05\", checkOut=\"2024-09-15\", geoId=\"foo\")\n</code></pre>"},{"location":"concepts/components/refraction/03.-The-Refraction-API-%7C-Tool-Calling/#314-recovery-call","title":"3.1.4 Recovery Call","text":"<p>Now we have messed up the call in a way that can be fixed by making an extra call! This will be allowed if you set the following parameter.</p> <pre><code>use_given_operators_only=False\n</code></pre> <pre><code>memory = {\n    \"query\": \"London\",\n}\n\npayload = {\n    \"geoId\": \"$var1.geoId$\",\n    \"checkIn\": \"2024-09-05\",\n    \"checkOut\": \"2024-09-15\",\n}\n\nhotel_details = search_hotels(\n     param=\"foo\", **payload, refractor=refractor, memory=memory,\n)\n</code></pre> <p>Notice the extra call! \ud83e\udd2f You will now get the result of executing the whole sequence in response to making the original faulty call.</p> <pre><code>+ var1 = TripadvisorSearchLocation(query=\"$query$\")\n- var1 = TripadvisorSearchHotels(param=\"foo\", geoId=\"$var1.geoId$\", checkIn=\"2024-09-05\", checkOut=\"2024-09-15\")\n?                                -------------\n\n+ var1 = TripadvisorSearchHotels(geoId=\"$var1.geoId$\", checkIn=\"2024-09-05\", checkOut=\"2024-09-15\")\n\nExecuting: var1 = TripadvisorSearchLocation(query=\"London\")\n\n\n  var1 = TripadvisorSearchLocation(query=\"London\")\n\nExecuting: TripadvisorSearchHotels(param=\"foo\", geoId=\"123\", checkIn=\"2024-09-05\", checkOut=\"2024-09-15\")\n</code></pre>"},{"location":"concepts/components/refraction/04.-Cost-Model-of-Edits/","title":"4.1 Debugging as an Optimization Problem","text":"<p>While introducing the concept of refraction, we talked about how we cast the debugging task as one of optimizing how many of the tokens in a sequence of API calls can be enforced given a set of API specifications. Of course, there may be multiple ways to fix an invalid token and hence the cost model used in the optimization process is crucial.</p> <p>In a previous chapter, we talked about two considerations in the refraction process: 1) the nature of the mistake being edited; and 2) the nature of the recovery response. Unsurprisingly, the cost model impacts both.</p>"},{"location":"concepts/components/refraction/04.-Cost-Model-of-Edits/#411-preferred-edits","title":"4.1.1 Preferred Edits","text":"<p>One of the critical considerations in trading off alternative fixes for a mistake is how we rank the preference of an edit in terms of whether it is a positive or negative edit, i.e. whether we want to fix stuff by adding or removing items; and which items positive and negative edits are operating on, e.g. are we editing the name of an API or a parameter within an API.</p> <p>Let us take the example of a missing label again and turn down the cost of a negative edit. In other words, this allows the debugger to remove stuff if it wants in order to make all the remaining tokens valid. The same input sequence now produces a delete operation on the problematic step and reuse an existing map, instead of trying to produce a new label that maintains the existing tokens.</p> <pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n- SkyScrapperSearchAirport(query=\"London\")\n- var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                                                                  ^                                                                     ^\n\n+ var3 = SkyScrapperFlightSearch(originSkyId=\"$var1.skyId$\", destinationSkyId=\"$var1.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var1.entityId$\", date=\"2024-08-15\")\n?                                                                                  ^                                                                     ^\n\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/04.-Cost-Model-of-Edits/#412-preferred-recovery-patterns","title":"4.1.2 Preferred Recovery Patterns","text":"<p>Similar trade-offs apply to recovery patterns. We saw previously in the case of the made up input parameter, how we can switch between slot-filling and mapping and function calling recovery modes on the same input.</p>"},{"location":"concepts/components/refraction/04.-Cost-Model-of-Edits/#slot-filling-preferred","title":"Slot-filling preferred","text":"<pre><code>+ ask(originSkyId)\n  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n- var3 = SkyScrapperFlightSearch(originalSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                      --      ^^^^ ^^^    --\n\n+ var3 = SkyScrapperFlightSearch(originSkyId=originSkyId, destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                            ^ ^^^^^\n\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre>"},{"location":"concepts/components/refraction/04.-Cost-Model-of-Edits/#internal-maps-preferred","title":"Internal maps preferred","text":"<pre><code>  var1 = SkyScrapperSearchAirport(query=\"New York\")\n  var2 = SkyScrapperSearchAirport(query=\"London\")\n- var3 = SkyScrapperFlightSearch(originalSkyId=\"$var1.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                      --           ^\n\n+ var3 = SkyScrapperFlightSearch(originSkyId=\"$var2.skyId$\", destinationSkyId=\"$var2.skyId$\", originEntityId=\"$var1.entityId$\", destinationEntityId=\"$var2.entityId$\", date=\"2024-08-15\")\n?                                                 ^\n\n  var4 = TripadvisorSearchLocation(query=\"London\")\n  var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n</code></pre> <p>As we have established so far, the results of refraction depends on the cost model of each edit both in terms of what edits are made and what recovery pattern is used in response to an edit. The refraction package currently operates under the following cost model:</p> <ol> <li>Edits on parameter names are preferred over function calls.</li> <li>Edits on parameter to value maps are preferred over edits to parameter names.</li> <li>New maps are considered at all and how they are preferred over other edits.</li> <li>Recovery operations include new maps, slot fills, and new functions calls.</li> </ol>"},{"location":"concepts/components/refraction/05.-Scaling/","title":"Scaling Characteristics","text":"<p>While introducing the concept of refraction, we talked about how one of the key considerations is response time. To reiterate, the edit call must be insignificant compared to a reasoning call to an LLM call. Otherwise, we might as well reason again by reflection.</p> <p>In the following sections, we will explore how the refraction API scales with different aspects of a sequence of API calls. These results are from running the debugger on the NESTFUL dataset, as described in more detail here.</p>"},{"location":"concepts/components/refraction/05.-Scaling/#51-counting-seconds","title":"5.1 Counting Seconds","text":"<p>\u26a0\ufe0f The times reported here are only to be treated relatively. Absolute times, of course, depend on the infrastructure running the code.</p>"},{"location":"concepts/components/refraction/05.-Scaling/#52-scaling-with-number-of-tokens","title":"5.2 Scaling with Number of Tokens","text":"<p>As we discussed previously here, the debugger decomposes the input sequence into a set of possible tokens to enforce. Hence, the complexity of the optimization step increases with the number of steps as well as the number of parameters in each step. However, based on the results below, the impact of this on the time to debug seems to be negligible compared to other factors.</p>"},{"location":"concepts/components/refraction/05.-Scaling/#521-time-taken-per-instance","title":"5.2.1 Time taken per instance","text":"<p>Average time: 0.678 secs</p> <p></p>"},{"location":"concepts/components/refraction/05.-Scaling/#522-time-taken-per-length-of-sequence","title":"5.2.2 Time taken per length of sequence","text":""},{"location":"concepts/components/refraction/05.-Scaling/#523-time-taken-per-length-of-sequence-x-parameters","title":"5.2.3 Time taken per length of sequence x parameters","text":""},{"location":"concepts/components/refraction/05.-Scaling/#524-time-taken-with-edit-distance","title":"5.2.4 Time taken with edit distance","text":"<p>Refraction on a set of tokens is likely to be faster if there are less edits to make. The intuition here is similar to how planning problems with longer solutions plans can be harder to solve more often than not. Hence, running the debugger to get a YES/NO signal (e.g. to send back for reflection) is likely to be faster than generating all the edits.</p>"},{"location":"concepts/components/refraction/05.-Scaling/#53-scaling-with-the-underlying-facts","title":"5.3 Scaling with the underlying facts","text":"<p>Apart from the input sequence itself, the other input to the debugger is the API specs or signatures. This has two possible impacts: 1) the size of the catalog i.e. the number of APIs; and 2) how coupled they are i.e. the number of possible mappings. These inputs seem to dominate the token size from the point of view of scaling.</p>"},{"location":"concepts/components/refraction/05.-Scaling/#531-scaling-with-the-size-of-the-catalog-with-no-mappings","title":"5.3.1 Scaling with the Size of the Catalog (with no mappings)","text":""},{"location":"concepts/components/refraction/05.-Scaling/#532-scaling-with-the-number-of-maps-with-all-apis","title":"5.3.2 Scaling with the Number of Maps (with all APIs)","text":""},{"location":"concepts/components/refraction/05.-Scaling/#54-impact-of-recovery-patterns-on-scaling","title":"5.4 Impact of Recovery Patterns on Scaling","text":"<p>Finally, the more recovery patterns the debugger has to reason about, the more time it takes. Primary among this is the presence of defensive actions because it effective doubles the number of edits (due to confirmations on newly introduced items in the sequence).</p>"},{"location":"concepts/components/refraction/05.-Scaling/#541-defensive-actions","title":"5.4.1 Defensive Actions","text":"Default With defensive actions Average time taken 0.678 secs 0.699 secs <p>Of course, since we are running on the ground truth at the moment, this effect is not that pronounced since there are hardly any edits to make. We will find out more in the future.</p>"},{"location":"concepts/components/refraction/06.-Refraction-on-NESTFUL/","title":"Refraction on the NESTFUL Dataset","text":"<p>All our examples so far has been using samples from the NESTFUL dataset. Of course, as ground truth, there are no mistakes to fix here. Nevertheless, we ran it on the data to measure some preliminary scaling characteristics, as well as catch some mistakes with the ground truth along the way! \ud83d\ude09</p>"},{"location":"concepts/components/refraction/06.-Refraction-on-NESTFUL/#61-executable-dataset","title":"6.1 Executable Dataset","text":"<p>Two (three?) kinds of errors were noticed quite frequently.</p>"},{"location":"concepts/components/refraction/06.-Refraction-on-NESTFUL/#wrong-parameter-name","title":"Wrong parameter name","text":"<p>For this example, the API <code>TripadvisorSearchRestaurants</code> is being called with wrong parameters. The sequence is invalid because of a missing required parameter.</p> <pre><code>  var1 = TripadvisorSearchLocation(query=\"Rome\")\n+ ask(locationId)\n- var2 = TripadvisorSearchRestaurants()\n+ var2 = TripadvisorSearchRestaurants(locationId=locationId)\n?                                     +++++++++++++++++++++\n</code></pre> <p>Similarly, for the following example, the API <code>Tripadvisor_Search_Restaurant_Location</code> does produce <code>location</code> and <code>location.locationId</code> but not directly <code>locationId</code>.</p> <pre><code>  var1 = Tripadvisor_Search_Restaurant_Location(query=\"Paris\")\n+ ask(locationId)\n- var2 = Tripadvisor_Search_Restaurants(locationId=\"$var1.locationId$\")\n?                                                  -------          --\n\n+ var2 = Tripadvisor_Search_Restaurants(locationId=locationId)\n  var3 = Tripadvisor_Get_Restaurant_Details(restaurantsId=\"$var2.restaurantsId$\")\n</code></pre>"},{"location":"concepts/components/refraction/06.-Refraction-on-NESTFUL/#assignment-to-non-existent-parameter","title":"Assignment to non-existent parameter","text":"<p>Another kind of frequent error is an assignment to an incorrect parameter of the result of an API call. In this example, the API <code>Spotify_Scraper_Get_Artist_ID_By_Name</code> does not produce an <code>id</code>.</p> <pre><code>  var1 = Spotify_Scraper_Get_Artist_ID_By_Name(name=\"Ed Sheeran\")\n+ var1 = Instagram_Search_User()\n  var2 = Spotify_Scraper_Get_Artist_Overview(artistId=\"$var1.id$\")\n</code></pre> <p>We have fixed several dozens (\ud83e\udd72) of such mistakes in the \"ground truth\" of NESTFUL using the refraction package. The newest version of the data is made available here and is also available on PyPI here.</p>"},{"location":"concepts/components/refraction/06.-Refraction-on-NESTFUL/#miscellaneous","title":"Miscellaneous \ud83d\ude48","text":"<p>We currently don't deal with operations on the parameters. So examples like these are false positives.</p> <pre><code>  var1 = Alpha_Vantage_CURRENCY_EXCHANGE_RATE(function=\"CURRENCY_EXCHANGE_RATE\", from_currency=\"INR\", to_currency=\"JPY\")\n- var2 = CipherCircuit_Math_Assistant_CalculateAllArithmeticOperations(numbers=\"100 * $var1.Exchange Rate$\")\n?                                                                               ------\n\n+ var2 = CipherCircuit_Math_Assistant_CalculateAllArithmeticOperations(numbers=\"$var1.Exchange Rate$\")\n</code></pre>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/","title":"Offline Analysis","text":"<p>So far, we have discussed how to refract on a single tool call or a sequence of tool calls. While this is useful at the time of execution, to generate immediate feedback, the output of refraction can also be used offline to improve model accuracy or even provide feedback to agent builders on where their agent is going wrong.</p> <p>Furthermore, we can do things offline that we cannot at runtime: e.g. 1) we can compute new metrics based on evaluation criterion (e.g. goodness of sequence with respect to the stated goals of a sequence); 2) identify redundant steps; 3) generate aggregate statistics on a set of observed calls.</p> <p> Got ideas for more fun stuff we can measure? Open an issue.</p>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#71-offline-basics","title":"7.1 Offline Basics","text":"<p>The basic API for running in offline-mode is the same as runtime. We already saw this once on the NESTFUL data here.</p> <p>However, the interpretation of the output is different from runtime usage. For example, in the following situation, the runtime response would be to repair the call(s) and execute by using the corrected call API here, or send it downstream to a reflector component as illustrated here. In offline mode, on the other hand, we need to cache this response as feedback to be used later.</p> <pre><code>  var4 = TripadvisorSearchLocation(query=\"London\")\n- var5 = TripadvisorSearchHotels(geoId=\"$var3.ID$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n?                                              ^\n\n+ var5 = TripadvisorSearchHotels(geoId=\"$var4.geoId$\", checkIn=\"2024-08-15\", checkOut=\"2024-08-18\")\n?                                           ^ +++ ^\n</code></pre>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#multiple-fixes","title":"Multiple Fixes","text":"<p>Some of the restriction of runtime usage, such as sub-second response, does not apply in the offline mode. So, even on the use cases that overlap with runtime usage, we can do more: such as allow for multiple or additional (computationally expensive but interesting) fixes, longer sequences, larger catalogs, more mappings, and so on. </p> <p>We can generated multiple fixes for the same problem like so:</p> <pre><code>from refraction.batch_actions import run_all_modes\n\nrun_all_modes(\n    sequence=[\n        {\"name\": \"Spotify_Scraper_Get_Artist_Overview\", \"arguments\": {\"artistId\": \"$var1.id$\"}}\n    ],\n    catalog=...,\n    mappings=...,\n    memory_objects={\n        \"var1\": {\n            \"artist_id\": 12345,\n        }\n    },\n)\n</code></pre> <p>Here, the reference in the assignment to the parameter <code>artistId</code> is messed up. The refraction call will refract the original tool call across different conditions and generate multiple possible fixes as feedback. Note in the following:</p> <ol> <li>The call can be fixed by using the reference <code>$var1.artist_id$</code> from memory instead.</li> <li>We can attempt to extract <code>artistId</code> directly from context / user utterance.</li> <li>We can make an extra function call <code>Spotify_Scraper_Get_Artist_Overview</code> to get the required information.</li> <li>Some of the conditions produce the same fix.</li> </ol> <pre><code>- var1 = Spotify_Scraper_Get_Artist_Overview(artistId=\"$var1.id$\")\n+ var1 = Spotify_Scraper_Get_Artist_Overview(artistId=\"$var1.artist_id$\")\n?                                                            +++++++\n</code></pre> <pre><code>- var1 = Spotify_Scraper_Get_Artist_Overview(artistId=\"$var1.id$\")\n+ var10 = Spotify_Scraper_Get_Artist_Overview(artistId=\"$var1.artist_id$\")\n?     +                                                       +++++++\n</code></pre> <pre><code>+ ask(artistId)\n- var1 = Spotify_Scraper_Get_Artist_Overview(artistId=\"$var1.id$\")\n?                                                       -  ^^\n\n+ var1 = Spotify_Scraper_Get_Artist_Overview(artistId=\"$artistId$\")\n?                                                         ^ +++\n</code></pre> <pre><code>+ ask(artistId)\n+ var1 = Spotify_Scraper_List_Artist_Albums_Singles(artistId=\"$artistId$\")\n  var1 = Spotify_Scraper_Get_Artist_Overview(artistId=\"$var1.id$\")\n</code></pre> <pre><code>+ ask(artistId)\n- var1 = Spotify_Scraper_Get_Artist_Overview(artistId=\"$var1.id$\")\n?                                                       -  ^^\n\n+ var1 = Spotify_Scraper_Get_Artist_Overview(artistId=\"$artistId$\")\n?                                                         ^ +++\n</code></pre> <p>For a set of calls, we can gather these insights into a report. More later on this later.</p>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#72-sequence-level-analysis","title":"7.2 Sequence-Level Analysis","text":"<p>In the offline-mode, we have additional options to play with, to decide if a sequence passes the refraction test or not, particularly in the context of the goodness of a sequence.</p> <p>From the perspective of the AI planning community, a plan can take up 3 forms:</p> <ol> <li>Sound (it is executable)</li> <li>Valid (it is sound + achieves whatever goals it was supposed to)</li> <li>Optimal (it is valid + the best of valid plans according to some metric)</li> </ol> <p>The metric in (3) is \"least cost\" by default, but to move from (1) to either of (2) or (3), we need goal annotations.</p>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#721-goal-annotations-for-testing","title":"7.2.1 Goal Annotations for Testing","text":"<p>At runtime, we only use the soundness check. This is because we want to allow an agent to do what it wants to (agent-knows-best mode), especially since we are not given an additional information on what is to be expected from a sequence.</p> <p>During offline analysis, we can allow for more stricter checks by allowing for goal specifications. This is particularly true if we already have ground truth data to check with or the developer is available to annotate their tests for more detailed analysis. For example, you can annotate a refraction call with goal annotations as follows:</p>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#operator-goals","title":"Operator Goals","text":"<p>An operator goal is a tool call that must be there in the input sequence.</p> <pre><code>from nl2flow.compile.schemas import Step\n\nresult = refract(\n    ...,\n    goals=[Step(name=\"SkyScrapperFlightSearch\")],\n)\n</code></pre>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#memory-goals","title":"Memory Goals","text":"<p>A memory goal is a variable that must be there in memory after completion of the input sequence.</p> <pre><code>from nl2flow.compile.schemas import MemoryItem\n\nresult = refract(\n    ...,\n    goals=[MemoryItem(item_id=\"flightId\")],\n)\n</code></pre>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#step-goals","title":"Step Goals","text":"<p>Operator goals can also be (partially or wholly) instantiated to indicate there must be a particular tool call in the input trajectory.</p> <pre><code>result = refract(\n    ...,\n    memory_objects={...},\n    goals=[\n        Step(\n            name=\"SkyScrapperFlightSearch\",\n            parameters=[\n                \"destinationSkyId\",\n                \"destinationEntityId\",\n                \"date\",\n            ],\n            maps=[\n                \"$var2.skyId$\",\n                \"$var2.entityId$\",\n                \"2024-08-18\",\n            ],\n        )\n    ],\n)\n</code></pre> <p>Once you have goal annotations, you can check how good your plan is with respect to those goals, beyond just the soundness check we have been doing so far.</p>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#722-validity","title":"7.2.2 Validity","text":"<p>By default, once you specify a goal in the input to refraction, it checks for validity.</p> <pre><code>result = refract(\n    sequence=[\n        {\n            \"name\": \"SkyScrapperSearchAirport\",\n            \"arguments\": {\"query\": \"New York\"},\n        },\n        {\n            \"name\": \"SkyScrapperSearchAirport\",\n            \"arguments\": {\"query\": \"San Juan\"},\n        },\n        {\n            \"name\": \"SkyScrapperFlightSearch\",\n            \"arguments\": {\n                \"originSkyId\": \"...\",\n                \"destinationSkyId\": \"...\",\n                \"originEntityId\": \"...\",\n                \"destinationEntityId\": \"...\",\n                \"date\": \"...\",\n            },\n        },\n    ],\n    catalog=...,\n    mappings=...,\n    goals=[MemoryItem(item_id=\"flightId\")],\n)\n\nassert result.report.determination\n</code></pre>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#723-optimality","title":"7.2.3 Optimality","text":"<p>You can also specify an optimality check like this. If not goals are specified, the refractor will assume that each mentioned tool call must appear at least once.</p> <pre><code>result = refract(\n    tool_calls,\n    tools,\n    goals=[Step(name=\"concur\")],\n    report_type=SolutionQuality.OPTIMAL,\n)\n</code></pre> <p>Consider the following tool calls. The corresponding tool specs are here. In the above call, we have required that this be an optimal sequence for the <code>concur</code> call.</p> <pre><code>[\n    {\n        \"name\": \"w3\",\n        \"arguments\": {\"email\": \"tchakra2@ibm.com\"},\n        \"label\": \"var1\",\n    },\n    {\n        \"name\": \"author_workbench\",\n        \"arguments\": {\"id\": \"$var1.id$\"},\n        \"label\": \"var2\",\n    },\n    {\n        \"name\": \"hr_bot\",\n        \"arguments\": {\"id\": \"$var1.id$\", \"email\": \"tchakra2@ibm.com\"},\n        \"label\": \"var3\",\n    },\n    {\n        \"name\": \"concur\",\n        \"arguments\": {\n            \"employee_info\": \"$var3.info$\",\n            \"travel_justification\": \"$var2.papers$\",\n        },\n        \"label\": \"var4\",\n    },\n]\n</code></pre> <p>Here the <code>hr_bot</code> can be called with a different optional input that allows us to reduce one call. In the refracted output shown below, the <code>w3</code> call has been removed, and the <code>hr_bot</code> call has been moved up. The <code>id</code> from this call is then reused for the call to <code>author_workbench</code> that has been moved further down. The final call to <code>concur</code> has been adjusted accordingly.</p> <pre><code>- var1 = w3(email=\"tchakra2@ibm.com\")\n?    ^   ^^\n\n+ var3 = hr_bot(email=\"tchakra2@ibm.com\")\n?    ^   ^^^^^^\n\n- var2 = author_workbench(id=\"$var1.id$\")\n?                                 ^\n\n+ var2 = author_workbench(id=\"$var3.id$\")\n?                                 ^\n\n- var3 = hr_bot(id=\"$var1.id$\", email=\"tchakra2@ibm.com\")\n- var4 = concur(employee_info=\"$var3.info$\", travel_justification=\"$var2.papers$\")\n?    ^\n\n+ var1 = concur(employee_info=\"$var3.info$\", travel_justification=\"$var2.papers$\")\n?    ^\n</code></pre>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#unnecessary-calls","title":"Unnecessary calls","text":"<p>In the previous example, we saw how the original plan all contributed to the goal but was suboptimal. It can also be that the plan has unnecessary steps that do not contribute to the goal at all.</p> <pre><code>result = refract(\n    tool_calls,\n    tools,\n    goals=[MemoryItem(item_id=\"employee_info\")],\n    report_type=SolutionQuality.OPTIMAL,\n)\n</code></pre> <p>The final step is removed and the call to <code>hr_bot</code> is adjusted to reflect that this is enough to achieve this memory goal.</p> <pre><code>- var1 = w3(email=\"tchakra2@ibm.com\")\n?    ^   ^^\n\n+ var3 = hr_bot(email=\"tchakra2@ibm.com\")\n?    ^   ^^^^^^\n\n- var2 = author_workbench(id=\"$var1.id$\")\n- var3 = hr_bot(id=\"$var1.id$\", email=\"tchakra2@ibm.com\")\n- var4 = concur(employee_info=\"$var3.info$\", travel_justification=\"$var2.papers$\")\n</code></pre>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#serendipity","title":"Serendipity","text":"<p>Finally, the plan will also adjust to serendipitous resolution of the state of the world. Consider that we have already executed the <code>hr_bot</code>, then the optimality call will again adjust the remaining call accordingly.</p> <pre><code>result = refract(\n    sequence=tool_calls,\n    catalog=tools,\n    memory_objects={\n        \"id\": 213213,\n        \"var1\": {\n            \"info\": \"...\",\n        },\n    },\n    goals=[Step(name=\"concur\")],\n    report_type=SolutionQuality.OPTIMAL,\n)\n</code></pre> <p>The refractor now removes the unnecessary call to <code>hr_bot</code>, reassigns the call to <code>author_workbench</code> to items already in memory, and adjusts the call to <code>concur</code> accordingly.</p> <pre><code>- var1 = w3(email=\"tchakra2@ibm.com\")\n- var2 = author_workbench(id=\"$var1.id$\")\n?                              -----\n\n+ var2 = author_workbench(id=\"$id$\")\n- var3 = hr_bot(id=\"$var1.id$\", email=\"tchakra2@ibm.com\")\n- var4 = concur(employee_info=\"$var3.info$\", travel_justification=\"$var2.papers$\")\n?    ^                             ^\n\n+ var1 = concur(employee_info=\"$var1.info$\", travel_justification=\"$var2.papers$\")\n?    ^                             ^\n</code></pre>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#73-compression-vs-refraction","title":"7.3 Compression vs Refraction","text":"<p>One of the interesting aspects of doing post-hoc analysis is that we do not have to follow the agent-knowns-best strategy, as mentioned before, anymore. Instead, we can look at an entire trajectory and look for stuff that need not be there. We call this compression. This is in fact one of the modes that trigger internally when we called refraction to run on all modes previously; but you can invoke it directly as well.</p> <pre><code>from refraction import compress\n\nresult = compress(\n    sequence=sequence, catalog=..., mappings=...\n)\n</code></pre> <p>Unlikely the refraction call that will preserve and attempt to fix all tool calls in a trajectory, the compression call will attempt to remove duplicate, and possibly corrupt, calls.</p>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#thrashing","title":"Thrashing","text":"<p>A special case of duplicates is when an agent is stuck in a sequence of bad calls. Consider this trajectory in from a ReAct agent trying to solve a NESTFUL task.</p> <pre><code>var1 = TripadvisorSearchLocation(query=\"Costa Rica\")\nvar2 = TripadvisorSearchHotels(geoId=\"$var1.geoId$\", checkIn=\"2024-12-01\", checkOut=\"2024-12-15\")\nvar3 = SkyScrapperSearchAirport(query=\"New York\")\nvar4 = SkyScrapperSearchAirport(query=\"Costa Rica\")\nvar5 = SkyScrapperFlightSearch(originSkyId=\"$var3.skyId$\", destinationSkyId=\"$var4.skyId$\", date=\"2024-12-01\")\nvar6 = SkyScrapperSearchAirport(query=\"New York\")\nvar7 = SkyScrapperFlightSearch(originEntityId=\"$var6.entityId$\", destinationSkyId=\"$var4.skyId$\", date=\"2024-12-01\")\nvar8 = SkyScrapperSearchAirport(query=\"New York\")\nvar9 = SkyScrapperFlightSearch(originSkyId=\"$var8.skyId$\", destinationSkyId=\"$var4.skyId$\", originEntityId=\"$var8.entityId$\", date=\"2024-12-01\")\n</code></pre> <p>There are several things to consider here for the compressor:</p> <ul> <li>Step producing <code>var4</code> should remain -- multiple correct tool invocations with different instantiation</li> <li>Step producing <code>var5</code> should go -- wrong parameters</li> <li>Step producing <code>var6</code> should go -- this is a correct call but unnecessarily repeated to support a failed call</li> <li>Step producing <code>var7</code> should go -- wrong parameters again, thrashing on wrong call</li> <li>Step producing <code>var8</code> should go -- back to thrashing on correct call to support a wrong call</li> <li>Step producing <code>var9</code> should go -- but either this or one of the previous wrong invocations needs to be fixed</li> </ul> <p>The compression call removes all the bad calls and consolidates a single correct call, absent in the original trajectory, as shown below. During offline analysis, you can use this to monitor thrashing behavior of an agent.</p> <pre><code>+ var7 = SkyScrapperSearchAirport(query=\"Costa Rica\")\n  var1 = TripadvisorSearchLocation(query=\"Costa Rica\")\n  var2 = TripadvisorSearchHotels(geoId=\"$var1.geoId$\", checkIn=\"2024-12-01\", checkOut=\"2024-12-15\")\n  var3 = SkyScrapperSearchAirport(query=\"New York\")\n+ var4 = SkyScrapperFlightSearch(originSkyId=\"$var7.skyId$\", destinationSkyId=\"$var3.skyId$\", date=\"2024-12-01\", originEntityId=\"$var7.entityId$\", destinationEntityId=\"$var3.entityId$\")\n- var4 = SkyScrapperSearchAirport(query=\"Costa Rica\")\n- var5 = SkyScrapperFlightSearch(originSkyId=\"$var3.skyId$\", destinationSkyId=\"$var4.skyId$\", date=\"2024-12-01\")\n- var6 = SkyScrapperSearchAirport(query=\"New York\")\n- var7 = SkyScrapperFlightSearch(originEntityId=\"$var6.entityId$\", destinationSkyId=\"$var4.skyId$\", date=\"2024-12-01\")\n- var8 = SkyScrapperSearchAirport(query=\"New York\")\n- var9 = SkyScrapperFlightSearch(originSkyId=\"$var8.skyId$\", destinationSkyId=\"$var4.skyId$\", originEntityId=\"$var8.entityId$\", date=\"2024-12-01\")\n</code></pre>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#75-aggregate-analysis","title":"7.5 Aggregate Analysis","text":"<p>With all these pieces in place, we can run aggregate analysis on a bunch of trajectories and produce 1) specific feedback per sample for offline improvement of models / agents, and 2) aggregate statistics highlighting areas for improvement.</p>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#751-error-tagging","title":"7.5.1 Error Tagging","text":"<p>In order to produce aggregate statistics the first step is to tag the errors in a tool call (this is done internally but if you ever need to tag elsewhere, you can do it like this).</p>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#tagging-a-single-call","title":"Tagging a single call","text":"<pre><code>from nestful import SequenceStep\nfrom nestful.errors import tag_sequence_step\n\ntool_call = {\"name\": \"TripadvisorSearchHotels\", \"arguments\": {\"geoId\": \"$var4.locationId$\", ...}}\ntagged_call = tag_sequence_step(SequenceStep(**tool_call), ground_truth=..., memory={...})\n</code></pre> <p>To reveal the error tags, inspect <code>tagged_call.errors</code>. Note that some errors might appear in multiple forms -- e.g. an assignment to a missing memory item can be counted as a wrong assignment as well (but not the other way round).</p> <pre><code>[\n    ErrorTag(error_type=&lt;ErrorType.WRONG_ASSIGNMENT: 'wrong_assignment'&gt;, info={'geoId': '$var4.locationId$'}),\n    ErrorTag(error_type=&lt;ErrorType.MADE_UP_ASSIGNMENT: 'made_up_assignment'&gt;, info='locationId'),\n    ErrorTag(error_type=&lt;ErrorType.MISSING_MEMORY: 'missing_memory'&gt;, info='$var4.locationId$')\n]\n</code></pre>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#tagging-a-sequence-object","title":"Tagging a sequence object","text":"<p>You can also tag an entire sequence together, which will reveal both step-level as well as sequence-level diffs.</p> <pre><code>from nestful import SequencingData\nfrom nestful.errors import tag_sequence\n\ntool_calls = [...]\ntagged_sequence = tag_sequence(\n    SequencingData(output=[SequenceStep(**tool_call) for call in tool_calls],\n    ground_truth=...,\n    memory={...},\n    catalog=...,\n)\n</code></pre> <p>To reveal the error tags, inspect <code>tagged_sequence.errors</code>.</p> <pre><code>[\n    ErrorTag(error_type=&lt;ErrorType.MADE_UP_API: 'made_up_api'&gt;, info='Tripadvisor_Search_Hotels'),\n    ErrorTag(error_type=&lt;ErrorType.NEW_CALL: 'new_call'&gt;, info='Tripadvisor_Search_Hotels'),\n    ErrorTag(error_type=&lt;ErrorType.WRONG_ASSIGNMENT: 'wrong_assignment'&gt;, info={'query': 'London'}),\n    ErrorTag(error_type=&lt;ErrorType.WRONG_ASSIGNMENT: 'wrong_assignment'&gt;, info={'query': 'New York'}),\n    ErrorTag(error_type=&lt;ErrorType.MADE_UP_ASSIGNMENT: 'made_up_assignment'&gt;, info='geoId'),\n    ErrorTag(error_type=&lt;ErrorType.MISSING_MEMORY: 'missing_memory'&gt;, info='$var4.geoId$'),\n    ErrorTag(error_type=&lt;ErrorType.WRONG_ASSIGNMENT: 'wrong_assignment'&gt;, info={'query': 'London'}),\n    ErrorTag(error_type=&lt;ErrorType.WRONG_ASSIGNMENT: 'wrong_assignment'&gt;, info={'query': 'New York'}),\n    ErrorTag(error_type=&lt;ErrorType.MADE_UP_ASSIGNMENT: 'made_up_assignment'&gt;, info='geoId'),\n    ErrorTag(error_type=&lt;ErrorType.MISSING_MEMORY: 'missing_memory'&gt;, info='$var4.geoId$')\n]\n</code></pre>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#752-report-generation","title":"7.5.2 Report Generation","text":"<p>Now we have everything in place to run the offline mode in batch and generate a report about the individual and aggregated goodness of the generated trajectories. There are multiple outcomes of report:</p> <ol> <li>For each trajectory, it should identify if errors occurred and of what type, and if they were fixable on the spot;</li> <li>Quality of the trajectory per their validity/optimality and possibility of compression;</li> <li>Aggregated pass/fail/quality statistics the entire dataset; and</li> <li>Per API/tool statistics for common failure modes.</li> </ol>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#run-as-sequence","title":"Run as sequence","text":"<p>The default batch runner rates a trajectory as if it were generated up front. Here, <code>data</code> is a list of 3-tuple containing a sequence, its ground truth (optional), and the catalog.</p> <pre><code>from refraction.batch_actions import run_all_batch\n\nresults: BatchResults = run_all_batch(data)\n</code></pre> <pre><code>Validating sequence 7/10\n\n\n  var1 = NewsAPISearchByKeyWord(query=\"UK sports news\", language=\"en\", region=\"GB\")\n  var2 = RedditTopPostsBySubreddit(subreddit=\"sports\", time=\"day\")\n\n\nValidating sequence 8/10\n\n\n  var1 = NewsAPISearchByKeyWord(query=\"India politics\", language=\"en\", region=\"IN\")\n  var2 = RedditTopPostsBySubreddit(subreddit=\"india\", time=\"week\")\n\n\nValidating sequence 9/10\nError generating step predicate: 'nlp.extract_topics'\nError generating step predicate: 'nlp.filter_posts'\n\n\n  var1 = NewsAPISearchByKeyWord(query=\"2024 US election\")\n- var2 = NLP.extract_topics(text=\"$var1.articles$\")\n- var3 = RedditTopPostsBySubreddit(subreddit=\"news\", time=\"day\", query=\"$var2.topics$\")\n?                                                              -----------------------\n\n+ var3 = RedditTopPostsBySubreddit(subreddit=\"news\", time=\"day\")\n- var4 = NLP.filter_posts(posts=\"$var3.posts$\", query=\"2024 US election\")\n+ var3 = NewsAPISearchByKeyWord(query=\"2024 US election\")\n\n\nValidating sequence 10/10\n\nAverage time taken: 3.42 sec\nSuccess Rate: 6/10\nCompression Rate: 0.11\nTroubled Indices: 0, 1, 3, 8\n\nTime taken: 34.28 secs\n</code></pre>"},{"location":"concepts/components/refraction/07.-Offline-Analysis/#run-step-by-step","title":"Run step-by-step","text":"<p>The above call ran post-hoc refraction on the whole sequence. You can also run batch analysis assuming the trajectory generates and executes one step at a time. This makes a difference if the output of each step is noisy. For example, a repeated tool call might be deemed unnecessary at the trajectory level but necessary in hindsight if a tool call failed in the middle. You can run the step by step mode like this.</p> <pre><code>run_all_batch(..., run_step_by_step=True)\n</code></pre>"},{"location":"concepts/components/refraction/08.-Integration-with-LangGraph/","title":"LangGraph ft. Refraction","text":"<p>Existing agentic frameworks rarely allow API calls directly, but allow access to tools as described above (which can in turn wrap an API call within). In a previous chapter, we explored how we can wrap those tools</p> <p>It turns out that the LangGraph tool decorator does not allow an extra wrapper around it. So you cannot currently use the refraction decorator to invoke a tool or agent directly. \u2639\ufe0f \ud83d\ude12 \ud83d\ude29</p> <p>However, you can still use it to call a tool node directly, or using the stateful formulation of a workflow.</p>"},{"location":"concepts/components/refraction/08.-Integration-with-LangGraph/#81-direct-tool-node-invocation","title":"8.1 Direct Tool Node Invocation","text":"<p>Consider the following tools: again\u2264 one directly with the input signature of the function as an API would have been, and the other with a state as input, but with information in the refraction decorator as to what API it wraps.</p> <pre><code>@refract(\n    api=\"TripadvisorSearchHotels\",\n    use_given_operators_only=False,\n    execute_if_fixed=True,\n    use_state=True,\n)\ndef search_hotels(state: State) -&gt; Dict[str, Any]:\n    \"\"\"Tripadvisor search hotels\"\"\"\n    return {\"response\": f\"{state.get('geoId')}: hotels\", **state}\n\n\n@refract()\ndef TripadvisorSearchLocation(query: str) -&gt; str:\n    \"\"\"Tripadvisor search location\"\"\"\n    return f\"{query}: locations\"\n</code></pre> <pre><code>catalog = get_nestful_catalog(executable=True)\nrefractor = Refractor(catalog)\n\ntools = [search_hotels, TripadvisorSearchLocation]\ntool_node = ToolNode(tools)\n\nmessage_with_single_tool_call = AIMessage(\n    content=\"\",\n    tool_calls=[\n        {\n            \"name\": \"TripadvisorSearchLocation\",\n            \"args\": {\"query\": \"San Juan\", \"refractor\": refractor},\n            \"id\": \"5241421\",\n            \"type\": \"tool_call\",\n        }\n    ],\n)\n\nresult = tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n</code></pre> <p>Notice the extra parameter in <code>args</code> (you can also pass in a memory like before). This call will pass without incident because the call is correct.</p>"},{"location":"concepts/components/refraction/08.-Integration-with-LangGraph/#82-calling-inside-a-workflow-using-state","title":"8.2 Calling Inside a Workflow using State","text":"<pre><code>workflow = StateGraph(State)\nworkflow.add_node(\"search_hotels_node\", search_hotels)\n\nworkflow.add_edge(start_key=START, end_key=\"search_hotels_node\")\nworkflow.add_edge(start_key=\"search_hotels_node\", end_key=END)\n\nchain = workflow.compile()\n\nmemory = {\"var1\": {\"geoId\": 123}}\n\nstate = chain.invoke(\n    {\n        \"checkIn\": \"...\",\n        \"checkOut\": \"...\",\n        \"refractor\": self.refractor,\n        \"memory\": memory,\n    }\n)\n</code></pre> <p>Here, the input for <code>geoId</code> is missing both in the input and in memory. The refractor decoration corrects the call using the memory input, and executes on the fly. \ud83d\ude0d</p> <pre><code>  var1 = TripadvisorSearchHotels(geoId=\"234\", checkIn=\"...\", checkOut=\"...\")\n\n\n- var1 = TripadvisorSearchHotels(checkIn=\"...\", checkOut=\"...\")\n+ var1 = TripadvisorSearchHotels(checkIn=\"...\", checkOut=\"...\", geoId=\"$var1.geoId$\")\n?                                                             ++++++++++++++++++++++\n</code></pre> <pre><code>Executing: TripadvisorSearchHotels(checkIn=\"...\", checkOut=\"...\", geoId=\"123\")\n</code></pre>"},{"location":"concepts/components/refraction/09.-Integration-with-Mellea/","title":"Mellea ft. Refraction","text":"<p>Mellea is an open-source library from IBM for writing generative programs. Mellea allows you to write structured instructions for your agents with off-the-shelf validators and prompting and sampling patterns.</p> <p>Refraction is a low-cost (no LLMs!), low-latency, domain-agnostic, data-agnostic, model-agnostic approach towards validation and repair for a sequence of tool calls.</p> <p>Naturally, a perfect fit. \ud83e\udd17</p> <p>The following is an example of an external integration. Since tool calling is one of the most dominant usage patterns of agentic systems, we would ideally like to make this integration available as a built-in requirement specialized for validating tool calling sequences.</p>"},{"location":"concepts/components/refraction/09.-Integration-with-Mellea/#91-mellea-requirements","title":"9.1 Mellea Requirements","text":"<p>The first point of integration is with Mellea requirements. These are in-built validators that operate on the output of a model to make sure things we care about are maintained in the output.</p> <p>For refraction, we care about the tool-calling part of the output -- this means, we want to make sure that the tool calls adhere to the tools specs, they flow of data between subsequent tool calls are following the specs, there is no redundant steps, there is no missing step, and so on. All the usual cool refraction stuff!</p> <p>For a simple example of passing the refraction requirement, try this test.</p> <pre><code>from refraction.integration.mellea_requirement import RefractionRequirement\n\nmellea_session = start_session()\nrefractor_req = RefractionRequirement(tools=tools)\n\ninference_result = mellea_session.instruct(\n    description=...,\n    requirements=[\n        refractor_req,\n    ],\n    user_variables={\n        \"query\": (\n            \"I need a travel approval to present my conference papers.\"\n            \" My email is tchakra2@ibm.com\"\n        ),\n        \"tools\": tools,\n        \"memory\": {},\n    },\n)\n\nassert inference_result.success is True\n</code></pre> <p>You can find the tools used for this example here, and the prompt here. The sequence of calls produced by the LLM, as shown below, passes the refraction test.</p> <pre><code>&lt;tool_calls&gt;[\n{\"name\": \"w3\", \"args\": {\"email\": \"tchakra2@ibm.com\"}, \"label\": \"var1\"},\n{\"name\": \"author_workbench\", \"args\": {\"id\": \"$var1.id\"}, \"label\": \"var2\"},\n{\"name\": \"concur\", \"args\": {\"employee_info\": \"$var1\", \"travel_justification\": [{\"paper\": \"$var2.papers\"}]}, \"label\": \"var3\"}\n]&lt;/tool_calls&gt;\n</code></pre>"},{"location":"concepts/components/refraction/09.-Integration-with-Mellea/#92-mellea-sampling","title":"9.2 Mellea Sampling","text":"<p>When the requirement is not met, refraction can do two things: 1) recommend a fixed sequence if possible; and 2) provide a new prompt as feedback to a new call to an LLM. For Mellea, this becomes a sampling step.</p> <p>\ud83d\udca1 This is the same two stage usage of refraction as described here, but now built into the Mellea instruct-validate-repair paradigm.</p> <p>In this test, we have intentionally messed up the LLM response to demonstrate this. We have removed a parameter in the call to the <code>concur</code> tool. The requirement responds by 1) flagging a failure; 2) computing a fixed call; and 3) a new prompt for the next sampling step (instead of retrying with the same prompt).</p> <pre><code>from refraction.integration.mellea_requirement import (\n    RefractionRequirement,\n    refract_repair,\n)\n\nmellea_session = start_session()\nrefractor_req = RefractionRequirement(tools=tools)\n\ninference_result = self.mellea_session.instruct(\n    description=...,\n    requirements=[\n        refractor_req,\n    ],\n    user_variables={\n        \"query\": ...,\n        \"tools\": tools,\n        \"memory\": {},\n    },\n    strategy=RejectionSamplingStrategy(\n        loop_budget=2, repair=refract_repair\n    ),\n    return_sampling_results=True,\n)\n\nassert inference_result.success is False\n</code></pre> <pre><code>  var1 = w3(email=\"tchakra2@ibm.com\")\n  var2 = author_workbench(id=\"$var1.id$\")\n  var3 = hr_bot(id=\"$var1.id$\", email=\"tchakra2@ibm.com\")\n- var4 = concur(travel_justification=\"$var2.papers$\")\n+ var4 = concur(employee_info=\"$var3.info$\", travel_justification=\"$var2.papers$\")\n?               +++++++++++++++++++++++++++++\n</code></pre> <pre><code>print(inference_result.sample_validations[0][0][1].reason)\n</code></pre> <pre><code>Please fix the provided tool call based on the issues outlined.\n\n&lt;tool_call&gt;[\n{'name': 'w3', 'arguments': {'email': 'tchakra2@ibm.com'}, 'label': 'var1'},\n{'name': 'author_workbench', 'arguments': {'id': '$var1.id$'}, 'label': 'var2'},\n{'name': 'hr_bot', 'arguments': {'id': '$var1.id$', 'email': 'tchakra2@ibm.com'}, 'label': 'var3'},\n{'name': 'concur', 'arguments': {'travel_justification': '$var2.papers$'}, 'label': 'var4'}\n]&lt;/tool_call&gt;\n\nThe following are the identified issues:\nEach issue is accompanied by guidance on how to fix it.\nConsider the guidance, along with the provided tool specs, and memory, to come up with the final fixed tool call.\n\n- Parameter employee_info is a required parameter for concur, but it is missing.\n- Possible fix: Get value of parameter employee_info by calling hr_bot.\n</code></pre> <p>For a range of feedback provided by the refractor, take a look at some cached prompts here.</p>"},{"location":"concepts/components/refraction/09.-Integration-with-Mellea/#93-gaps-for-a-more-perfect-union","title":"9.3 Gaps for a more perfect union","text":"<p>While the above examples are enough to get us started, we could do much more (and have a much slicker dev interface to the refraction requirement) if the following were possible:</p> <ul> <li> <p>I could not see any immediate method to access the prompt / instruction (and not just the response) inside the requirement. This should be possible, and then we would not require any extra inputs (i.e. tools, memory, etc.) at all to instantiate the requirement -- we can read off everything from the instruction. [link]</p> </li> <li> <p>Currently, the validation result only allows for a <code>str</code> reason. This means that the refractor, in the event it could fix a call in situ, as in the example above, it must use the prompting mechanism for resampling. An alternative pathway could be to allow for return the fix in the validator, if it is able to fix in place, and this saves us an extra sampling cost + avoid the possibility of ending up with a new error in the new sample. [link]</p> </li> </ul>"},{"location":"concepts/components/refraction/10.-Integration-with-LangFlow/","title":"10. Integration with LangFlow","text":"<pre><code>\u231b  Direct integration attempt with Langflow pending open-source.\n</code></pre> <p>Langflow is a powerful tool for building and deploying AI-powered agents and workflows. It provides developers with both a visual authoring experience and built-in API and MCP servers that turn every workflow into a tool that can be integrated into applications built on any framework or stack. Langflow comes with batteries included and supports all major LLMs, vector databases and a growing library of AI tools.</p> <p>Langflow is now part of the IBM family! \ud83e\udd17</p>"},{"location":"concepts/components/refraction/10.-Integration-with-LangFlow/#refraction-as-a-component","title":"Refraction as a component","text":"<p>You can add refraction as a component. Until we are open-source, you have to do it yourself. Please follow the instructions here, and use the file here as an example. Once you have done so, it will appear in the list of components.</p> <p></p> <ul> <li>You can either pass in a list of tool calls directly, or dump in all the tool specs, tool calls, memory, etc. in the instruction as a text input.</li> <li>The output of the component is the entire refraction output, as described here, which you can use in the rest of your flow however you desire.</li> </ul> <p></p> <p></p>"},{"location":"examples/","title":"Examples","text":"<ol> <li>Spotlight Jupyter Notebook</li> <li>SPARC Example</li> <li>JSON Processor Jupyter Notebook</li> <li>RAG Repair Jupyter Notebook</li> <li>Silent Review Jupyter Notebook</li> <li>Policy Guard Jupyter Notebook</li> </ol>"},{"location":"getting_started/","title":"Getting started","text":"<p>ALTK is designed to give developers the tools to quickly build agents and solve problems at each stage in an agent's lifecycle. These guides can help you try out the available components in no time at all.</p> \u2b07\ufe0f InstallationQuickly install ALTK in your environment \u25b6\ufe0f UsageStart using components in your agent \ud83d\udcad ConceptsGet in-depth details about how components work \ud83e\uddd1\u200d\ud83c\udf73 ExamplesTry out recipes for various components and use cases \ud83d\udd0c IntegrationsCheck out integrations with popular AI tools and frameworks"},{"location":"installation/","title":"Installation","text":"<p>To use the Agent Lifecycle Toolkit, simply install <code>altk</code> from your Python package manager, e.g. pip: <pre><code>pip install altk\n</code></pre></p> <p>Works on macOS, Linux, and Windows, with support for both x86_64 and arm64 architectures.</p>"},{"location":"installation/#development-setup","title":"Development setup","text":"<p>To develop ALTK features, bugfixes etc., install as follows from your local clone's root dir:</p> <pre><code>uv sync --all-extras\n</code></pre> <p>You may also be interested in taking a look at our contributing guidelines.</p>"},{"location":"integrations/","title":"Integrations","text":"<p>ALTK is designed to integrate flexibly into agentic pipelines, and its components can be configured in multiple ways depending on the target environment.</p>"},{"location":"integrations/#mcp","title":"MCP","text":"<p>A notable integration is with the ContextForge MCP Gateway, which allows ALTK components to be configured externally \u2014 without modifying the agent code. This separation of concerns enables teams to experiment with lifecycle enhancements, enforce policies, and improve reliability without touching the agent\u2019s core logic. For example, components like SPARC, or Silent Review can be activated or tuned via configuration, making it easier for agents to benefit from these components.</p> <p>See a demo here of how tools responses with large JSON payloads can be handled reliably without polluting your agent's context.</p>"},{"location":"integrations/#langflow","title":"Langflow","text":"<p>ALTK also works well with Langflow, a visual programming interface for LLM agents. Developers can compose workflows and drop an agent with configurable ALTK components using Langflow\u2019s visual interface to easily experiment with different configurations and understand how ALTK components affect agent behavior.</p> <p>Here is a demo of a custom ALTK agent in Langflow with ALTK components integrated to improve tool calling.</p>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#pre-llm","title":"Pre-LLM","text":"<p>For immediately before the prompt is sent to the model</p> \ufe0f\u2728 Spotlight     Emphasize important spans within your prompts to steer attention"},{"location":"usage/#pre-tool","title":"Pre-tool","text":"<p>For immediately before a tool is invoked by an agent</p> \ufe0f\ud83d\udca1 Refraction     Validate and repair tool call sequences using classical AI planning    \ufe0f\u26a1\ufe0f SPARC     Evaluate the static and semantic elements tool calls before execution"},{"location":"usage/#post-tool","title":"Post-tool","text":"<p>For immediately after a tool response is received</p> \ud83d\udcda JSON Processor     Dynamically extract data from large and complex JSON objects    \ud83d\udcc4\ud83d\udee0\ufe0f RAG Repair     Use domain documentation to repair tool call errors    \ufe0f\ud83d\udcac Silent Review     Useful for correcting tool calls where errors are not explicit in the response"},{"location":"usage/#pre-response","title":"Pre-response","text":"<p>For immediately before the agent returns a response to the user</p> \ud83d\udee1\ufe0f Policy Guard     Guard and enforce policies in LLM responses"},{"location":"usage/post_tool/","title":"Index","text":""},{"location":"usage/post_tool/#pre-llm","title":"Pre-LLM","text":"<ul> <li>Spotlight</li> </ul>"},{"location":"usage/post_tool/#pre-tool","title":"Pre-tool","text":"<ul> <li>Refraction</li> <li>SPARC</li> </ul>"},{"location":"usage/post_tool/#post-tool","title":"Post-tool","text":"<ul> <li>JSON Processor</li> <li>RAG Repair</li> <li>Silent Review</li> </ul>"},{"location":"usage/post_tool/#pre-response","title":"Pre-response","text":"<ul> <li>Policy Guard</li> </ul>"},{"location":"usage/post_tool/json_processor/","title":"Json processor","text":"<p>JSON PROCESSOR!</p>"},{"location":"usage/post_tool/rag_repair/","title":"Rag repair","text":"<p>RAG REPAIR!</p>"},{"location":"usage/post_tool/silent_review/","title":"Silent review","text":"<p>SILENT REVIEW!</p>"},{"location":"usage/pre_llm/","title":"Index","text":"<p>links</p>"},{"location":"usage/pre_llm/spotlight/","title":"Spotlight","text":"<p>Spotlight!</p>"},{"location":"usage/pre_response/","title":"Index","text":"<p>links</p>"},{"location":"usage/pre_response/policy_guard/","title":"Policy guard","text":"<p>POLICY GUARD!</p>"},{"location":"usage/pre_tool/","title":"Index","text":"<p>links</p>"},{"location":"usage/pre_tool/refraction/","title":"Refraction","text":"<p>REFRACTION!</p>"},{"location":"usage/pre_tool/sparc/","title":"Sparc","text":"<p>SPARC!</p>"}]}